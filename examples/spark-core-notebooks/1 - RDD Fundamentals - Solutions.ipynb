{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: RDD Fundamentals - Solutions\n",
    "\n",
    "**Objective**: Learn the basics of Resilient Distributed Datasets (RDDs) - Spark's fundamental data structure.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Understand RDD properties: immutable, distributed, resilient\n",
    "- Create RDDs from collections and data sources\n",
    "- Apply basic transformations and actions\n",
    "- Explore RDD lineage and fault tolerance\n",
    "\n",
    "**Estimated Time**: 45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's set up our Spark environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nimport pandas as pd\nimport time\n\n# Initialize Spark Session\nspark = SparkSession.builder \\\n    .appName(\"Lab1-RDD-Fundamentals\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Get Spark Context\nsc = spark.sparkContext\n\nprint(f\"Spark version: {spark.version}\")\nui_url = spark.sparkContext.uiWebUrl\nprint(f\"Spark UI available at: {ui_url}\")\nprint(\"ðŸ’¡ In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating RDDs\n",
    "\n",
    "RDDs can be created in several ways. Let's explore the most common methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating RDDs from Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple RDD from a Python list\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(f\"RDD created with {numbers_rdd.getNumPartitions()} partitions\")\n",
    "print(f\"First few elements: {numbers_rdd.take(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Create an RDD from a list of your favorite programming languages and specify 3 partitions.\n",
    "\n",
    "*Hint: Use `sc.parallelize()` with the `numSlices` parameter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create languages_rdd from a list of programming languages with 3 partitions\n",
    "languages = [\"Python\", \"Scala\", \"Java\", \"R\", \"SQL\", \"JavaScript\"]\n",
    "languages_rdd = sc.parallelize(languages, numSlices=3)\n",
    "\n",
    "# Validation\n",
    "assert languages_rdd.getNumPartitions() == 3, \"RDD should have 3 partitions\"\n",
    "assert len(languages_rdd.collect()) >= 3, \"Should have at least 3 languages\"\n",
    "print(\"âœ“ Exercise 1.1 completed successfully!\")\n",
    "print(f\"Languages RDD: {languages_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating RDDs from Files\n",
    "\n",
    "Let's load our synthetic customer data as an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data as text file (RDD of strings)\n",
    "customers_text_rdd = sc.textFile(\"../Datasets/customers.csv\")\n",
    "\n",
    "print(f\"Number of lines in customer file: {customers_text_rdd.count()}\")\n",
    "print(f\"First line (header): {customers_text_rdd.first()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2**: Load the customer transactions file and skip the header line.\n",
    "\n",
    "*Hint: Use `filter()` or slice the RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Load transactions file and remove header\n",
    "transactions_rdd = sc.textFile(\"../Datasets/customer_transactions.csv\")\n",
    "header = transactions_rdd.first()\n",
    "transactions_no_header = transactions_rdd.filter(lambda line: line != header)\n",
    "\n",
    "# Validation\n",
    "assert transactions_no_header.count() > 0, \"Should have transaction records\"\n",
    "assert header not in transactions_no_header.collect()[:10], \"Header should be removed\"\n",
    "print(\"âœ“ Exercise 1.2 completed successfully!\")\n",
    "print(f\"Transactions (without header): {transactions_no_header.count()} records\")\n",
    "print(f\"Sample record: {transactions_no_header.take(1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Transformations\n",
    "\n",
    "Transformations are lazy operations that define a new RDD based on existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Map Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply map transformation to square numbers\n",
    "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
    "\n",
    "print(f\"Original: {numbers_rdd.collect()}\")\n",
    "print(f\"Squared: {squared_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Parse the transaction CSV lines into structured data.\n",
    "\n",
    "*Create an RDD where each element is a dictionary with keys: transaction_id, customer_id, amount, category*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transaction(line):\n",
    "    \"\"\"Parse a CSV line into a transaction dictionary\"\"\"\n",
    "    # Solution: Split the line and create a dictionary\n",
    "    # Expected columns: transaction_id,customer_id,amount,category,payment_method,transaction_date,is_weekend,discount_applied\n",
    "    fields = line.split(',')\n",
    "    return {\n",
    "        'transaction_id': fields[0],\n",
    "        'customer_id': fields[1],\n",
    "        'amount': float(fields[2]),\n",
    "        'category': fields[3]\n",
    "    }\n",
    "\n",
    "# Solution: Apply the parsing function to create structured transaction RDD\n",
    "structured_transactions = transactions_no_header.map(parse_transaction)\n",
    "\n",
    "# Validation\n",
    "sample_transaction = structured_transactions.first()\n",
    "assert isinstance(sample_transaction, dict), \"Should return dictionaries\"\n",
    "assert 'transaction_id' in sample_transaction, \"Should have transaction_id key\"\n",
    "assert isinstance(sample_transaction['amount'], float), \"Amount should be float\"\n",
    "print(\"âœ“ Exercise 2.1 completed successfully!\")\n",
    "print(f\"Sample parsed transaction: {sample_transaction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for even numbers\n",
    "even_numbers = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(f\"Even numbers: {even_numbers.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**: Filter transactions for high-value purchases (amount > $100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Filter for transactions with amount greater than 100\n",
    "high_value_transactions = structured_transactions.filter(lambda t: t['amount'] > 100)\n",
    "\n",
    "# Validation\n",
    "sample_high_value = high_value_transactions.take(5)\n",
    "for transaction in sample_high_value:\n",
    "    assert transaction['amount'] > 100, f\"Amount {transaction['amount']} should be > 100\"\n",
    "\n",
    "print(\"âœ“ Exercise 2.2 completed successfully!\")\n",
    "print(f\"High-value transactions: {high_value_transactions.count()} out of {structured_transactions.count()}\")\n",
    "print(f\"Percentage: {(high_value_transactions.count() / structured_transactions.count()) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 FlatMap Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Split words and flatten\n",
    "sentences = [\"Hello world\", \"Spark is awesome\", \"RDDs are cool\"]\n",
    "sentences_rdd = sc.parallelize(sentences)\n",
    "\n",
    "# Map vs FlatMap comparison\n",
    "words_map = sentences_rdd.map(lambda sentence: sentence.split(\" \"))\n",
    "words_flatmap = sentences_rdd.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "\n",
    "print(f\"Using map: {words_map.collect()}\")\n",
    "print(f\"Using flatMap: {words_flatmap.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: Extract all unique categories from transactions using flatMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Extract categories and get unique values\n",
    "categories_rdd = structured_transactions.map(lambda t: t['category'])\n",
    "unique_categories = categories_rdd.distinct()\n",
    "\n",
    "# Validation\n",
    "categories_list = unique_categories.collect()\n",
    "assert len(categories_list) > 0, \"Should have at least one category\"\n",
    "assert len(categories_list) == len(set(categories_list)), \"Should be unique categories\"\n",
    "print(\"âœ“ Exercise 2.3 completed successfully!\")\n",
    "print(f\"Unique categories: {sorted(categories_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Basic Actions\n",
    "\n",
    "Actions trigger the execution of transformations and return results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Collect and Count Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different actions\n",
    "print(f\"Count: {numbers_rdd.count()}\")\n",
    "print(f\"First element: {numbers_rdd.first()}\")\n",
    "print(f\"Take 3: {numbers_rdd.take(3)}\")\n",
    "print(f\"All elements: {numbers_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Calculate basic statistics for transaction amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Extract amounts and calculate statistics\n",
    "amounts_rdd = structured_transactions.map(lambda t: t['amount'])\n",
    "\n",
    "total_count = amounts_rdd.count()\n",
    "total_amount = amounts_rdd.reduce(lambda a, b: a + b)\n",
    "min_amount = amounts_rdd.min()\n",
    "max_amount = amounts_rdd.max()\n",
    "avg_amount = total_amount / total_count\n",
    "\n",
    "# Validation\n",
    "assert total_count > 0, \"Should have transactions\"\n",
    "assert total_amount > 0, \"Total amount should be positive\"\n",
    "assert min_amount <= avg_amount <= max_amount, \"Statistics should be consistent\"\n",
    "\n",
    "print(\"âœ“ Exercise 3.1 completed successfully!\")\n",
    "print(f\"Transaction Statistics:\")\n",
    "print(f\"  Count: {total_count:,}\")\n",
    "print(f\"  Total Amount: ${total_amount:,.2f}\")\n",
    "print(f\"  Average Amount: ${avg_amount:.2f}\")\n",
    "print(f\"  Min Amount: ${min_amount:.2f}\")\n",
    "print(f\"  Max Amount: ${max_amount:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reduce Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find maximum number using reduce\n",
    "maximum = numbers_rdd.reduce(lambda a, b: max(a, b))\n",
    "sum_total = numbers_rdd.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(f\"Maximum: {maximum}\")\n",
    "print(f\"Sum: {sum_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2**: Find the customer with the highest total spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Group by customer and find highest spender\n",
    "# Step 1: Create (customer_id, amount) pairs\n",
    "customer_amounts = structured_transactions.map(lambda t: (t['customer_id'], t['amount']))\n",
    "\n",
    "# Step 2: Sum amounts by customer\n",
    "customer_totals = customer_amounts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 3: Find customer with maximum total\n",
    "top_customer = customer_totals.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "\n",
    "# Validation\n",
    "assert isinstance(top_customer, tuple), \"Should return a tuple (customer_id, amount)\"\n",
    "assert len(top_customer) == 2, \"Should have customer_id and amount\"\n",
    "assert top_customer[1] > 0, \"Top customer should have positive spending\"\n",
    "\n",
    "print(\"âœ“ Exercise 3.2 completed successfully!\")\n",
    "print(f\"Top customer: {top_customer[0]} with total spending: ${top_customer[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RDD Lineage and Persistence\n",
    "\n",
    "Understanding RDD lineage is crucial for fault tolerance and performance optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Examining Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a chain of transformations\npipeline_rdd = numbers_rdd \\\n    .filter(lambda x: x > 5) \\\n    .map(lambda x: x * 2) \\\n    .filter(lambda x: x < 20)\n\nprint(f\"Final result: {pipeline_rdd.collect()}\")\nprint(f\"Lineage information:\")\n# Fixed: Use toDebugString() to show lineage instead of accessing dependencies directly\nprint(f\"Debug string:\\n{pipeline_rdd.toDebugString().decode('utf-8')}\")\nprint(f\"Number of partitions: {pipeline_rdd.getNumPartitions()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1**: Create a processing pipeline and examine its lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create a processing pipeline with multiple transformations\n",
    "electronics_pipeline = structured_transactions \\\n",
    "    .filter(lambda t: t['category'] == 'Electronics') \\\n",
    "    .filter(lambda t: t['amount'] > 50) \\\n",
    "    .map(lambda t: t['amount'])\n",
    "\n",
    "# Execute and examine lineage\n",
    "result_count = electronics_pipeline.count()\n",
    "sample_amounts = electronics_pipeline.take(5)\n",
    "\n",
    "# Validation\n",
    "assert result_count > 0, \"Should have some electronics transactions > $50\"\n",
    "for amount in sample_amounts:\n",
    "    assert amount > 50, f\"Amount {amount} should be > $50\"\n",
    "\n",
    "print(\"âœ“ Exercise 4.1 completed successfully!\")\n",
    "print(f\"Electronics transactions > $50: {result_count}\")\n",
    "print(f\"Sample amounts: {sample_amounts}\")\n",
    "print(f\"Pipeline lineage:\\n{electronics_pipeline.toDebugString().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Persistence and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate caching benefits\n",
    "import time\n",
    "\n",
    "# Create expensive computation\n",
    "expensive_rdd = structured_transactions.filter(lambda t: t['amount'] > 100).map(lambda t: t['amount'])\n",
    "\n",
    "# Time without caching\n",
    "start_time = time.time()\n",
    "count1 = expensive_rdd.count()\n",
    "sum1 = expensive_rdd.sum()\n",
    "time_without_cache = time.time() - start_time\n",
    "\n",
    "# Cache and time with caching\n",
    "expensive_rdd.cache()\n",
    "start_time = time.time()\n",
    "count2 = expensive_rdd.count()  # This will cache the RDD\n",
    "sum2 = expensive_rdd.sum()      # This will use the cache\n",
    "time_with_cache = time.time() - start_time\n",
    "\n",
    "print(f\"Without cache: {time_without_cache:.3f} seconds\")\n",
    "print(f\"With cache: {time_with_cache:.3f} seconds\")\n",
    "print(f\"Results - Count: {count1}, Sum: {sum1:.2f}\")\n",
    "\n",
    "# Clean up cache\n",
    "expensive_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2**: Implement caching for a reusable computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create an RDD that will be used multiple times and cache it\n",
    "frequently_used_rdd = structured_transactions \\\n",
    "    .filter(lambda t: t['amount'] > 25) \\\n",
    "    .map(lambda t: (t['category'], t['amount']))\n",
    "\n",
    "# Solution: Cache the RDD\n",
    "frequently_used_rdd.cache()\n",
    "\n",
    "# Perform multiple actions to benefit from caching\n",
    "total_records = frequently_used_rdd.count()\n",
    "categories = frequently_used_rdd.map(lambda x: x[0]).distinct().collect()\n",
    "avg_by_category = frequently_used_rdd.groupByKey().mapValues(lambda amounts: sum(amounts) / len(amounts)).collect()\n",
    "\n",
    "# Validation\n",
    "assert total_records > 0, \"Should have cached records\"\n",
    "assert len(categories) > 0, \"Should have categories\"\n",
    "assert frequently_used_rdd.is_cached, \"RDD should be cached\"\n",
    "\n",
    "print(\"âœ“ Exercise 4.2 completed successfully!\")\n",
    "print(f\"Cached RDD statistics:\")\n",
    "print(f\"  Total records: {total_records}\")\n",
    "print(f\"  Categories: {categories}\")\n",
    "print(f\"  Is cached: {frequently_used_rdd.is_cached}\")\n",
    "\n",
    "# Clean up\n",
    "frequently_used_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Working with Key-Value RDDs\n",
    "\n",
    "Key-value RDDs enable powerful operations like joins and grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Creating Key-Value RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create key-value RDD from transactions\n",
    "category_amounts = structured_transactions.map(lambda t: (t['category'], t['amount']))\n",
    "\n",
    "print(f\"Sample key-value pairs: {category_amounts.take(5)}\")\n",
    "print(f\"Keys only: {category_amounts.keys().distinct().collect()}\")\n",
    "print(f\"Values only: {category_amounts.values().take(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.1**: Analyze spending patterns by customer and category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create customer-category spending analysis\n",
    "# Step 1: Create ((customer_id, category), amount) pairs\n",
    "customer_category_pairs = structured_transactions.map(lambda t: \n",
    "    ((t['customer_id'], t['category']), t['amount'])\n",
    ")\n",
    "\n",
    "# Step 2: Sum amounts by customer-category combination\n",
    "customer_category_totals = customer_category_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 3: Find top spending customer-category combinations\n",
    "top_combinations = customer_category_totals.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "\n",
    "# Validation\n",
    "assert len(top_combinations) > 0, \"Should have customer-category combinations\"\n",
    "for combo in top_combinations[:3]:\n",
    "    assert len(combo[0]) == 2, \"Key should be (customer_id, category) tuple\"\n",
    "    assert combo[1] > 0, \"Amount should be positive\"\n",
    "\n",
    "print(\"âœ“ Exercise 5.1 completed successfully!\")\n",
    "print(\"Top 10 Customer-Category Spending Combinations:\")\n",
    "for i, ((customer, category), amount) in enumerate(top_combinations, 1):\n",
    "    print(f\"  {i:2d}. {customer} - {category}: ${amount:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 GroupByKey and ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare groupByKey vs reduceByKey\nfrom pyspark.sql.functions import col\n\n# Using reduceByKey (more efficient)\ncategory_totals_reduce = category_amounts.reduceByKey(lambda a, b: a + b)\n\n# Using groupByKey (less efficient)\ncategory_totals_group = category_amounts.groupByKey().mapValues(lambda amounts: sum(amounts))\n\nprint(\"Category totals using reduceByKey:\")\nfor category, total in category_totals_reduce.collect():\n    print(f\"  {category}: ${total:.2f}\")\n\n# Verify they produce the same results (with rounding to handle floating point precision)\nreduce_results = {k: round(v, 2) for k, v in category_totals_reduce.collect()}\ngroup_results = {k: round(v, 2) for k, v in category_totals_group.collect()}\nassert reduce_results == group_results, \"Both methods should produce same results\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.2**: Calculate average transaction amount by category using both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Method 1: Using groupByKey\n# Solution: Calculate average amounts by category using groupByKey\ncategory_averages_group = category_amounts.groupByKey().mapValues(lambda amounts: \n    sum(amounts) / len(amounts)\n)\n\n# Method 2: Using reduceByKey (more efficient approach)\n# Solution: Calculate average using sum and count\ncategory_sums = category_amounts.reduceByKey(lambda a, b: a + b)\ncategory_counts = category_amounts.mapValues(lambda x: 1).reduceByKey(lambda a, b: a + b)\n\n# Join sums and counts to calculate averages\ncategory_averages_reduce = category_sums.join(category_counts).mapValues(lambda x: x[0] / x[1])\n\n# Collect results with rounding to handle floating point precision\naverages_group = {k: round(v, 2) for k, v in category_averages_group.collect()}\naverages_reduce = {k: round(v, 2) for k, v in category_averages_reduce.collect()}\n\n# Validation\nassert len(averages_group) > 0, \"Should have category averages\"\nfor category in averages_group:\n    assert abs(averages_group[category] - averages_reduce[category]) < 0.01, \"Both methods should produce similar results\"\n\nprint(\"âœ“ Exercise 5.2 completed successfully!\")\nprint(\"Average transaction amounts by category:\")\nfor category, avg in sorted(averages_group.items()):\n    print(f\"  {category}: ${avg:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Cleanup\n",
    "\n",
    "Congratulations! You've completed Lab 1 on RDD Fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts Learned:\n",
    "\n",
    "1. **RDD Creation**: From collections and files\n",
    "2. **Transformations**: map, filter, flatMap (lazy evaluation)\n",
    "3. **Actions**: collect, count, reduce (trigger execution)\n",
    "4. **Lineage**: Understanding RDD dependencies and fault tolerance\n",
    "5. **Persistence**: Caching for performance optimization\n",
    "6. **Key-Value Operations**: reduceByKey, groupByKey, join\n",
    "\n",
    "### Performance Tips:\n",
    "- Use `reduceByKey` instead of `groupByKey` when possible\n",
    "- Cache RDDs that will be used multiple times\n",
    "- Minimize data shuffling operations\n",
    "- Use appropriate partitioning for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark session\n",
    "spark.stop()\n",
    "print(\"Lab 1 completed successfully! ðŸŽ‰\")\n",
    "print(\"Next: Lab 2 - Transformations vs Actions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}