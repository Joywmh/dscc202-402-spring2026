{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Lazy Evaluation Deep Dive - Solutions\n",
    "\n",
    "**Objective**: Master Spark's lazy evaluation system, DAG optimization, and execution planning.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Understand how Spark builds and optimizes execution DAGs\n",
    "- Explore the Catalyst optimizer and query planning\n",
    "- Analyze job stages, tasks, and scheduling\n",
    "- Implement optimization techniques using lazy evaluation\n",
    "- Debug performance issues through execution analysis\n",
    "\n",
    "**Estimated Time**: 55 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark import SparkContext, SparkConf\nimport time\nimport json\nimport pandas as pd\n\n# Configure Spark for detailed execution analysis\nconf = SparkConf() \\\n    .setAppName(\"Lab3-Lazy-Evaluation\") \\\n    .set(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .set(\"spark.sql.adaptive.logLevel\", \"ERROR\") \\\n    .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")  # Suppress warnings for cleaner output\nspark.sparkContext.setLogLevel(\"ERROR\")  # Extra safety for log suppression\n\nprint(f\"üöÄ Spark {spark.version} initialized\")\n\n# Enhanced Spark UI URL display\nui_url = spark.sparkContext.uiWebUrl\nprint(f\"üìä Spark UI: {ui_url}\")\nprint(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")\n\nprint(f\"üÜî Application: {spark.sparkContext.applicationId}\")\nprint(f\"üîß Default parallelism: {sc.defaultParallelism}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding DAG Construction\n",
    "\n",
    "Explore how Spark builds Directed Acyclic Graphs (DAGs) from transformation chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic DAG Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for analysis\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "\n",
    "# Load as DataFrames (uses Catalyst optimizer)\n",
    "customers_df = spark.read.csv(\"../Datasets/customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"../Datasets/customer_transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(f\"‚úì Customers: {customers_df.count():,} records\")\n",
    "print(f\"‚úì Transactions: {transactions_df.count():,} records\")\n",
    "\n",
    "# Show schemas\n",
    "print(\"\\nüìã Customer Schema:\")\n",
    "customers_df.printSchema()\n",
    "print(\"\\nüìã Transaction Schema:\")\n",
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a complex transformation chain (no execution yet!)\nprint(\"üîó Building transformation chain...\")\n\n# Step 1: Filter high-value transactions\nhigh_value_transactions = transactions_df.filter(transactions_df.amount > 100)\nprint(\"‚úì Step 1: Filter transformation defined\")\n\n# Step 2: Group by category and calculate statistics\nfrom pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count as spark_count\n\ncategory_stats = high_value_transactions.groupBy(\"category\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_avg(\"amount\").alias(\"avg_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    )\nprint(\"‚úì Step 2: GroupBy transformation defined\")\n\n# Step 3: Join with customer data\ncustomer_transaction_join = high_value_transactions.join(\n    customers_df, \n    high_value_transactions.customer_id == customers_df.customer_id,\n    \"inner\"\n)\nprint(\"‚úì Step 3: Join transformation defined\")\n\n# Step 4: Calculate customer spending patterns\ncustomer_spending = customer_transaction_join.groupBy(\"state\", \"category\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_spent\"),\n        spark_count(\"*\").alias(\"purchase_count\")\n    )\nprint(\"‚úì Step 4: Complex aggregation defined\")\n\nprint(\"\\nüéØ All transformations defined - NO execution yet!\")\nprint(\"üìù DAG is built in memory, ready for optimization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Examining Execution Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze execution plans before triggering actions\n",
    "print(\"üîç Examining execution plans...\\n\")\n",
    "\n",
    "# Logical plan (what we want to do)\n",
    "print(\"üìã LOGICAL PLAN:\")\n",
    "print(customer_spending.explain(mode=\"simple\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà OPTIMIZED PHYSICAL PLAN:\")\n",
    "print(customer_spending.explain(mode=\"extended\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Build and analyze your own complex transformation DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Create a complex analysis pipeline and examine its execution plan\n# Requirements:\n# 1. Filter customers by age range (25-45)\n# 2. Join with transactions\n# 3. Calculate average transaction amount by payment method and state\n# 4. Filter for states with > $10,000 total transactions\n# 5. Sort by total amount descending\n\nfrom pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count as spark_count\n\nprint(\"üîß Building your analysis pipeline...\")\n\n# Step 1: Filter customers by age\ntarget_customers = customers_df.filter((customers_df.age >= 25) & (customers_df.age <= 45))\nprint(\"‚úì Age filter defined\")\n\n# Step 2: Join with transactions\ncustomer_transactions = target_customers.join(transactions_df, \"customer_id\", \"inner\")\nprint(\"‚úì Customer-transaction join defined\")\n\n# Step 3: Calculate payment method statistics by state\npayment_analysis = customer_transactions.groupBy(\"state\", \"payment_method\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_avg(\"amount\").alias(\"avg_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    )\nprint(\"‚úì Payment method analysis defined\")\n\n# Step 4: Filter for high-volume states\nhigh_volume_states = payment_analysis.filter(payment_analysis.total_amount > 10000)\nprint(\"‚úì High-volume filter defined\")\n\n# Step 5: Sort by total amount\nfinal_analysis = high_volume_states.orderBy(high_volume_states.total_amount.desc())\nprint(\"‚úì Sorting transformation defined\")\n\nprint(\"\\nüéØ Complex pipeline built! Examining execution plan...\")\n\n# Display the execution plan\nprint(\"\\nüìã EXECUTION PLAN:\")\nfinal_analysis.explain()\n\n# Validation by executing a simple action\nresult_count = final_analysis.count()\nsample_results = final_analysis.take(3)\n\nassert result_count > 0, \"Should have analysis results\"\nprint(f\"\\n‚úì Exercise 1.1 completed! Found {result_count} state-payment combinations\")\nprint(f\"üìä Top result: {sample_results[0] if sample_results else 'No results'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Catalyst Optimizer in Action\n",
    "\n",
    "Explore how Spark's Catalyst optimizer transforms and optimizes queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Predicate Pushdown Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate predicate pushdown optimization\n",
    "print(\"üîß Demonstrating Catalyst optimizations...\\n\")\n",
    "\n",
    "# Suboptimal query pattern (filter after join)\n",
    "print(\"‚ùå SUBOPTIMAL: Filter after expensive join\")\n",
    "suboptimal_query = transactions_df.join(\n",
    "    customers_df,\n",
    "    transactions_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").filter(\n",
    "    (transactions_df.amount > 200) & \n",
    "    (customers_df.state == \"CA\")\n",
    ")\n",
    "\n",
    "print(\"Suboptimal execution plan:\")\n",
    "suboptimal_query.explain(True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Optimal query pattern (Catalyst will optimize both to be the same!)\n",
    "print(\"‚úÖ OPTIMAL: Catalyst pushes filters down automatically\")\n",
    "optimal_query = transactions_df.filter(transactions_df.amount > 200).join(\n",
    "    customers_df.filter(customers_df.state == \"CA\"),\n",
    "    transactions_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"Optimized execution plan:\")\n",
    "optimal_query.explain(True)\n",
    "\n",
    "# Time both approaches\n",
    "print(\"\\n‚è±Ô∏è  Performance comparison:\")\n",
    "\n",
    "start_time = time.time()\n",
    "suboptimal_count = suboptimal_query.count()\n",
    "suboptimal_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_count = optimal_query.count()\n",
    "optimal_time = time.time() - start_time\n",
    "\n",
    "print(f\"Suboptimal approach: {suboptimal_count} records in {suboptimal_time:.4f}s\")\n",
    "print(f\"Optimal approach: {optimal_count} records in {optimal_time:.4f}s\")\n",
    "print(f\"üìä Both should be similar due to Catalyst optimization!\")\n",
    "\n",
    "assert suboptimal_count == optimal_count, \"Both queries should return same results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Column Pruning and Projection Pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate column pruning optimization\n",
    "print(\"‚úÇÔ∏è  Column Pruning Optimization\\n\")\n",
    "\n",
    "# Query that only needs specific columns\n",
    "print(\"üìã Query selecting only needed columns:\")\n",
    "efficient_query = transactions_df.select(\"customer_id\", \"amount\", \"category\") \\\n",
    "    .filter(transactions_df.amount > 100) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg({\"amount\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(amount)\", \"avg_amount\")\n",
    "\n",
    "print(\"Execution plan (note column pruning):\")\n",
    "efficient_query.explain()\n",
    "\n",
    "# Query that selects all columns but only uses a few\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìã Query with unnecessary column selection:\")\n",
    "inefficient_query = transactions_df.select(\"*\") \\\n",
    "    .filter(transactions_df.amount > 100) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg({\"amount\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(amount)\", \"avg_amount\")\n",
    "\n",
    "print(\"Execution plan (Catalyst still optimizes):\")\n",
    "inefficient_query.explain()\n",
    "\n",
    "# Execute both and compare\n",
    "print(\"\\n‚è±Ô∏è  Timing comparison:\")\n",
    "\n",
    "start_time = time.time()\n",
    "efficient_result = efficient_query.collect()\n",
    "efficient_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "inefficient_result = inefficient_query.collect()\n",
    "inefficient_time = time.time() - start_time\n",
    "\n",
    "print(f\"Efficient query: {len(efficient_result)} results in {efficient_time:.4f}s\")\n",
    "print(f\"'Inefficient' query: {len(inefficient_result)} results in {inefficient_time:.4f}s\")\n",
    "print(f\"üìà Performance similar due to Catalyst's column pruning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Compare optimized vs unoptimized query patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Create queries that demonstrate Catalyst optimizations\nprint(\"üß™ Testing Catalyst Optimization Patterns\\n\")\n\n# Import Spark functions to avoid conflicts\nfrom pyspark.sql.functions import sum as spark_sum, count as spark_count\n\n# Pattern 1: Join with filters (test predicate pushdown)\nprint(\"üîç Pattern 1: Predicate Pushdown Test\")\n\n# Unoptimized pattern - filter after join\nunopt_join_filter = transactions_df.join(\n    customers_df,\n    \"customer_id\",\n    \"inner\"\n).filter(\n    (transactions_df.amount > 150) & (customers_df.age > 30)\n)\n\n# Optimized pattern - filter before join\nopt_join_filter = transactions_df.filter(transactions_df.amount > 150).join(\n    customers_df.filter(customers_df.age > 30),\n    \"customer_id\",\n    \"inner\"\n)\n\nprint(\"Unoptimized plan (filter after join):\")\nunopt_join_filter.explain()\n\nprint(\"\\nOptimized plan (filter before join):\")\nopt_join_filter.explain()\n\n# Pattern 2: Aggregation with unnecessary columns\nprint(\"\\n\" + \"=\"*50)\nprint(\"üîç Pattern 2: Column Pruning Test\")\n\n# Query with explicit column selection\nexplicit_columns = transactions_df.select(\"category\", \"amount\", \"payment_method\") \\\n    .groupBy(\"category\", \"payment_method\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    )\n\n# Query with select all (Catalyst should optimize)\nimplicit_columns = transactions_df.select(\"*\") \\\n    .groupBy(\"category\", \"payment_method\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    )\n\n# Time both approaches\nprint(\"\\n‚è∞ Performance Comparison:\")\n\nstart_time = time.time()\nunopt_count = unopt_join_filter.count()\nunopt_time = time.time() - start_time\n\nstart_time = time.time()\nopt_count = opt_join_filter.count()\nopt_time = time.time() - start_time\n\nstart_time = time.time()\nexplicit_count = explicit_columns.count()\nexplicit_time = time.time() - start_time\n\nstart_time = time.time()\nimplicit_count = implicit_columns.count()\nimplicit_time = time.time() - start_time\n\nprint(f\"Join patterns:\")\nprint(f\"  Unoptimized: {unopt_count} records in {unopt_time:.4f}s\")\nprint(f\"  Optimized: {opt_count} records in {opt_time:.4f}s\")\nprint(f\"\\nColumn patterns:\")\nprint(f\"  Explicit columns: {explicit_count} records in {explicit_time:.4f}s\")\nprint(f\"  Implicit columns: {implicit_count} records in {implicit_time:.4f}s\")\n\n# Validation\nassert unopt_count == opt_count, \"Join patterns should return same count\"\nassert explicit_count == implicit_count, \"Column patterns should return same count\"\n\nprint(\"\\n‚úì Exercise 2.1 completed! Catalyst optimizes both patterns.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create queries with different complexity to analyze stages\nprint(\"üìä Analyzing Job Stages and Task Distribution\\n\")\n\n# Simple query - no shuffle required\nprint(\"üü¢ Simple Query (No Shuffle):\")\nsimple_query = transactions_df.filter(transactions_df.amount > 100) \\\n    .select(\"customer_id\", \"amount\", \"category\")\n\nprint(\"Plan for simple query:\")\nsimple_query.explain()\n\n# Execute and measure\nprint(\"\\nExecuting simple query...\")\nstart_time = time.time()\nsimple_result = simple_query.count()\nsimple_exec_time = time.time() - start_time\nprint(f\"Result: {simple_result} records in {simple_exec_time:.4f}s\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# Complex query - requires shuffle\nprint(\"üî¥ Complex Query (With Shuffle):\")\ncomplex_query = transactions_df.join(customers_df, \"customer_id\") \\\n    .groupBy(\"state\", \"category\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    ) \\\n    .orderBy(\"total_amount\", ascending=False)\n\nprint(\"Plan for complex query:\")\ncomplex_query.explain()\n\n# Execute and measure\nprint(\"\\nExecuting complex query...\")\nstart_time = time.time()\ncomplex_result = complex_query.count()\ncomplex_exec_time = time.time() - start_time\nprint(f\"Result: {complex_result} records in {complex_exec_time:.4f}s\")\n\nprint(f\"\\nüìà Performance Analysis:\")\nprint(f\"Simple query (no shuffle): {simple_exec_time:.4f}s\")\nprint(f\"Complex query (with shuffle): {complex_exec_time:.4f}s\")\nprint(f\"Complexity overhead: {complex_exec_time/simple_exec_time:.1f}x slower\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Analyzing Job Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create queries with different complexity to analyze stages\nprint(\"üìä Analyzing Job Stages and Task Distribution\\n\")\n\n# Simple query - no shuffle required\nprint(\"üü¢ Simple Query (No Shuffle):\")\nsimple_query = transactions_df.filter(transactions_df.amount > 100) \\\n    .select(\"customer_id\", \"amount\", \"category\")\n\nprint(\"Plan for simple query:\")\nsimple_query.explain()\n\n# Execute and measure\nprint(\"\\nExecuting simple query...\")\nstart_time = time.time()\nsimple_result = simple_query.count()\nsimple_exec_time = time.time() - start_time\nprint(f\"Result: {simple_result} records in {simple_exec_time:.4f}s\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# Complex query - requires shuffle\nprint(\"üî¥ Complex Query (With Shuffle):\")\ncomplex_query = transactions_df.join(customers_df, \"customer_id\") \\\n    .groupBy(\"state\", \"category\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    ) \\\n    .orderBy(\"total_amount\", ascending=False)\n\nprint(\"Plan for complex query:\")\ncomplex_query.explain()\n\n# Execute and measure\nprint(\"\\nExecuting complex query...\")\nstart_time = time.time()\ncomplex_result = complex_query.count()\ncomplex_exec_time = time.time() - start_time\nprint(f\"Result: {complex_result} records in {complex_exec_time:.4f}s\")\n\nprint(f\"\\nüìà Performance Analysis:\")\nprint(f\"Simple query (no shuffle): {simple_exec_time:.4f}s\")\nprint(f\"Complex query (with shuffle): {complex_exec_time:.4f}s\")\nprint(f\"Complexity overhead: {complex_exec_time/simple_exec_time:.1f}x slower\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding Shuffle Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different types of operations that cause shuffles\n",
    "print(\"üîÄ Understanding Shuffle Operations\\n\")\n",
    "\n",
    "# Operations that DON'T cause shuffles (narrow transformations)\n",
    "print(\"‚úÖ Operations WITHOUT Shuffle:\")\n",
    "no_shuffle_ops = [\n",
    "    (\"filter\", transactions_df.filter(transactions_df.amount > 50)),\n",
    "    (\"select\", transactions_df.select(\"customer_id\", \"amount\")),\n",
    "    (\"withColumn\", transactions_df.withColumn(\"amount_doubled\", transactions_df.amount * 2)),\n",
    "    (\"map (via RDD)\", transactions_df.rdd.map(lambda row: (row.customer_id, row.amount)).toDF([\"customer\", \"amount\"]))\n",
    "]\n",
    "\n",
    "for op_name, op_df in no_shuffle_ops:\n",
    "    print(f\"\\n{op_name.upper()} operation:\")\n",
    "    op_df.explain()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Operations that DO cause shuffles (wide transformations)\n",
    "print(\"‚ö†Ô∏è  Operations WITH Shuffle:\")\n",
    "shuffle_ops = [\n",
    "    (\"groupBy\", transactions_df.groupBy(\"category\").count()),\n",
    "    (\"orderBy\", transactions_df.orderBy(\"amount\", ascending=False)),\n",
    "    (\"join\", transactions_df.join(customers_df, \"customer_id\")),\n",
    "    (\"distinct\", transactions_df.select(\"customer_id\").distinct())\n",
    "]\n",
    "\n",
    "for op_name, op_df in shuffle_ops:\n",
    "    print(f\"\\n{op_name.upper()} operation:\")\n",
    "    op_df.explain()\n",
    "    \n",
    "print(\"\\nüìù Key Insight: Look for 'Exchange' operations in plans - these indicate shuffles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Design queries to minimize shuffle operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Create optimized versions of common query patterns\nprint(\"üéØ Shuffle Minimization Challenge\\n\")\n\n# Import Spark functions\nfrom pyspark.sql.functions import sum as spark_sum, count as spark_count\n\n# Challenge 1: Top customers by spending (minimize shuffles)\nprint(\"ü•á Challenge 1: Find top 10 customers by total spending\")\n\n# Approach A: Traditional approach (may have multiple shuffles)\napproach_a = transactions_df.groupBy(\"customer_id\") \\\n    .agg(spark_sum(\"amount\").alias(\"total_spent\")) \\\n    .orderBy(\"total_spent\", ascending=False) \\\n    .limit(10)\n\nprint(\"\\nApproach A (traditional):\")\napproach_a.explain()\n\n# Approach B: Optimized approach\napproach_b = transactions_df.filter(transactions_df.amount > 50) \\\n    .groupBy(\"customer_id\") \\\n    .agg(spark_sum(\"amount\").alias(\"total_spent\")) \\\n    .orderBy(\"total_spent\", ascending=False) \\\n    .limit(10)\n\nprint(\"\\nApproach B (optimized):\")\napproach_b.explain()\n\n# Challenge 2: Category analysis by state (minimize data movement)\nprint(\"\\n\" + \"=\"*50)\nprint(\"ü•à Challenge 2: Category spending by customer state\")\n\n# Approach A: Join then aggregate\njoin_then_agg = transactions_df.join(customers_df, \"customer_id\") \\\n    .groupBy(\"state\", \"category\") \\\n    .agg(\n        spark_sum(\"amount\").alias(\"total_amount\"),\n        spark_count(\"*\").alias(\"transaction_count\")\n    )\n\nprint(\"\\nJoin-then-aggregate approach:\")\njoin_then_agg.explain()\n\n# Approach B: Aggregate then join (potentially more efficient)\nagg_then_join = transactions_df.groupBy(\"customer_id\", \"category\") \\\n    .agg(spark_sum(\"amount\").alias(\"customer_category_total\")) \\\n    .join(customers_df.select(\"customer_id\", \"state\"), \"customer_id\") \\\n    .groupBy(\"state\", \"category\") \\\n    .agg(spark_sum(\"customer_category_total\").alias(\"total_amount\"))\n\nprint(\"\\nAggregate-then-join approach:\")\nagg_then_join.explain()\n\n# Performance comparison\nprint(\"\\n‚è±Ô∏è  Performance Comparison:\")\n\ntimings = {}\n\n# Time Approach A (Challenge 1)\nstart = time.time()\nresult_a1 = approach_a.count()\ntimings['top_customers_traditional'] = time.time() - start\n\n# Time Approach B (Challenge 1)\nstart = time.time()\nresult_b1 = approach_b.count()\ntimings['top_customers_optimized'] = time.time() - start\n\n# Time Approach A (Challenge 2)\nstart = time.time()\nresult_a2 = join_then_agg.count()\ntimings['category_join_then_agg'] = time.time() - start\n\n# Time Approach B (Challenge 2)\nstart = time.time()\nresult_b2 = agg_then_join.count()\ntimings['category_agg_then_join'] = time.time() - start\n\nprint(f\"Challenge 1 Results:\")\nprint(f\"  Traditional: {result_a1} results in {timings['top_customers_traditional']:.4f}s\")\nprint(f\"  Optimized: {result_b1} results in {timings['top_customers_optimized']:.4f}s\")\n\nprint(f\"\\nChallenge 2 Results:\")\nprint(f\"  Join-then-aggregate: {result_a2} results in {timings['category_join_then_agg']:.4f}s\")\nprint(f\"  Aggregate-then-join: {result_b2} results in {timings['category_agg_then_join']:.4f}s\")\n\n# Validation\nassert result_a1 <= 10, \"Should return at most 10 top customers\"\nassert result_a2 > 0, \"Should have category results\"\n\nprint(\"\\n‚úì Exercise 3.1 completed! Analyzed shuffle optimization patterns.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Lazy Evaluation Patterns\n",
    "\n",
    "Explore advanced techniques for leveraging lazy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Conditional Execution Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate lazy evaluation for conditional processing\nprint(\"üîÄ Conditional Execution with Lazy Evaluation\\n\")\n\n# Import Spark functions for aggregations\nfrom pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count as spark_count\n\ndef analyze_customer_segments(min_transaction_amount=50, top_n=10):\n    \"\"\"Conditional analysis based on parameters\"\"\"\n    \n    print(f\"üîç Analyzing segments with min_amount=${min_transaction_amount}, top_n={top_n}\")\n    \n    # Base transformation (always applied)\n    base_analysis = transactions_df.filter(transactions_df.amount >= min_transaction_amount)\n    \n    # Conditional transformations (only defined if needed)\n    if min_transaction_amount > 100:\n        # High-value analysis\n        analysis = base_analysis.join(customers_df, \"customer_id\") \\\n            .groupBy(\"state\", \"category\") \\\n            .agg(\n                spark_sum(\"amount\").alias(\"total\"),\n                spark_avg(\"amount\").alias(\"average\")\n            )\n        print(\"üìä High-value analysis pipeline created\")\n    else:\n        # Standard analysis\n        analysis = base_analysis.groupBy(\"category\") \\\n            .agg(\n                spark_sum(\"amount\").alias(\"total\"),\n                spark_count(\"*\").alias(\"count\")\n            )\n        print(\"üìà Standard analysis pipeline created\")\n    \n    # Final transformation (conditionally applied)\n    if top_n > 0:\n        final_result = analysis.orderBy(\"total\", ascending=False).limit(top_n)\n        print(f\"üéØ Limited to top {top_n} results\")\n    else:\n        final_result = analysis.orderBy(\"total\", ascending=False)\n        print(\"üìã All results included\")\n    \n    # Execution happens here!\n    print(\"\\n‚ö° Executing analysis...\")\n    start_time = time.time()\n    results = final_result.collect()\n    execution_time = time.time() - start_time\n    \n    print(f\"‚úÖ Completed: {len(results)} results in {execution_time:.4f}s\")\n    return results, execution_time\n\n# Test different scenarios\nprint(\"üß™ Testing different analysis scenarios:\\n\")\n\nscenarios = [\n    (\"Low threshold\", 25, 5),\n    (\"High threshold\", 150, 10),\n    (\"No limit\", 75, 0)\n]\n\nresults_summary = []\nfor scenario_name, min_amount, limit in scenarios:\n    print(f\"üìã Scenario: {scenario_name}\")\n    results, exec_time = analyze_customer_segments(min_amount, limit)\n    results_summary.append((scenario_name, len(results), exec_time))\n    print(f\"   Sample result: {results[0] if results else 'No results'}\\n\")\n\nprint(\"üìä Scenario Performance Summary:\")\nfor name, count, time_taken in results_summary:\n    print(f\"  {name}: {count} results in {time_taken:.4f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pipeline Reuse and Caching Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate intelligent caching with lazy evaluation\nprint(\"üíæ Smart Caching with Lazy Evaluation\\n\")\n\n# Import required Spark functions\nfrom pyspark.sql.functions import when, sum as spark_sum, count as spark_count, avg as spark_avg\n\nclass AnalyticsPipeline:\n    def __init__(self):\n        self.cached_stages = {}\n        \n    def get_enriched_transactions(self):\n        \"\"\"Lazy-loaded enriched transaction data\"\"\"\n        if 'enriched' not in self.cached_stages:\n            print(\"üîß Creating enriched transactions pipeline...\")\n            enriched = transactions_df.join(customers_df, \"customer_id\") \\\n                .withColumn(\"amount_category\", \n                    when(transactions_df.amount < 50, \"low\")\n                    .when(transactions_df.amount < 200, \"medium\")\n                    .otherwise(\"high\")\n                ) \\\n                .cache()  # Cache this expensive join\n            \n            self.cached_stages['enriched'] = enriched\n            print(\"‚úÖ Enriched pipeline cached\")\n        else:\n            print(\"‚ôªÔ∏è  Using cached enriched pipeline\")\n            \n        return self.cached_stages['enriched']\n    \n    def analyze_by_state(self):\n        \"\"\"State-level analysis using cached base\"\"\"\n        print(\"\\nüìä State Analysis:\")\n        enriched = self.get_enriched_transactions()\n        \n        result = enriched.groupBy(\"state\", \"amount_category\") \\\n            .agg(\n                spark_sum(\"amount\").alias(\"total_amount\"),\n                spark_count(\"*\").alias(\"transaction_count\")\n            )\n        \n        return result.collect()\n    \n    def analyze_by_category(self):\n        \"\"\"Category analysis using same cached base\"\"\"\n        print(\"\\nüìà Category Analysis:\")\n        enriched = self.get_enriched_transactions()\n        \n        result = enriched.groupBy(\"category\", \"amount_category\") \\\n            .agg(\n                spark_avg(\"amount\").alias(\"avg_amount\"),\n                spark_avg(\"age\").alias(\"avg_customer_age\")\n            )\n        \n        return result.collect()\n    \n    def cleanup(self):\n        \"\"\"Clean up cached data\"\"\"\n        for stage_name, stage_df in self.cached_stages.items():\n            stage_df.unpersist()\n            print(f\"üßπ Cleaned up {stage_name} cache\")\n\n# Demonstrate pipeline reuse\nprint(\"üöÄ Testing pipeline with intelligent caching:\")\npipeline = AnalyticsPipeline()\n\n# First analysis - will create and cache base data\nstart_time = time.time()\nstate_results = pipeline.analyze_by_state()\nstate_time = time.time() - start_time\nprint(f\"State analysis: {len(state_results)} results in {state_time:.4f}s\")\n\n# Second analysis - will reuse cached base data\nstart_time = time.time()\ncategory_results = pipeline.analyze_by_category()\ncategory_time = time.time() - start_time\nprint(f\"Category analysis: {len(category_results)} results in {category_time:.4f}s\")\n\n# Third analysis - will again reuse cache\nstart_time = time.time()\nstate_results_2 = pipeline.analyze_by_state()\nstate_time_2 = time.time() - start_time\nprint(f\"State analysis (2nd run): {len(state_results_2)} results in {state_time_2:.4f}s\")\n\nprint(f\"\\nüìä Caching Benefits:\")\nprint(f\"First state analysis: {state_time:.4f}s (builds cache)\")\nprint(f\"Category analysis: {category_time:.4f}s (uses cache)\")\nprint(f\"Second state analysis: {state_time_2:.4f}s (uses cache)\")\nprint(f\"Cache speedup: {state_time / state_time_2:.1f}x faster\")\n\n# Cleanup\npipeline.cleanup()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1**: Design an advanced analytics pipeline with smart caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Create a sophisticated analytics pipeline with multiple cached stages\nprint(\"üèóÔ∏è  Advanced Analytics Pipeline Challenge\\n\")\n\n# Import required functions\nfrom pyspark.sql.functions import when, col, sum as spark_sum, avg as spark_avg, count as spark_count\n\nclass CustomerInsightsPipeline:\n    def __init__(self):\n        self.cache_stages = {}\n        self.execution_metrics = {}\n    \n    def get_base_customer_data(self):\n        \"\"\"Base customer enrichment - cache this expensive operation\"\"\"\n        if 'base_customer' not in self.cache_stages:\n            print(\"üî® Building base customer data...\")\n            \n            customer_summaries = transactions_df.groupBy(\"customer_id\") \\\n                .agg(\n                    spark_sum(\"amount\").alias(\"total_spent\"),\n                    spark_count(\"*\").alias(\"transaction_count\"),\n                    spark_avg(\"amount\").alias(\"avg_amount\")\n                )\n            \n            base_data = customers_df.join(customer_summaries, \"customer_id\") \\\n                .withColumn(\"spending_segment\",\n                    when(col(\"total_spent\") < 1000, \"low\")\n                    .when(col(\"total_spent\") < 5000, \"medium\")\n                    .otherwise(\"high\")\n                ) \\\n                .withColumn(\"age_group\",\n                    when(col(\"age\") < 25, \"18-25\")\n                    .when(col(\"age\") < 35, \"26-35\")\n                    .when(col(\"age\") < 45, \"36-45\")\n                    .otherwise(\"45+\")\n                ) \\\n                .cache()\n            \n            self.cache_stages['base_customer'] = base_data\n            print(\"‚úÖ Base customer data cached\")\n        else:\n            print(\"‚ôªÔ∏è  Reusing cached base customer data\")\n        \n        return self.cache_stages['base_customer']\n    \n    def get_transaction_features(self):\n        \"\"\"Transaction-level features - another cacheable stage\"\"\"\n        if 'transaction_features' not in self.cache_stages:\n            print(\"üî® Building transaction features...\")\n            \n            base_customers = self.get_base_customer_data()\n            \n            features = transactions_df.join(base_customers, \"customer_id\") \\\n                .withColumn(\"is_high_value\", \n                    col(\"amount\") > col(\"avg_amount\")\n                ) \\\n                .withColumn(\"customer_lifetime_ratio\",\n                    col(\"amount\") / col(\"total_spent\")\n                ) \\\n                .cache()\n            \n            self.cache_stages['transaction_features'] = features\n            print(\"‚úÖ Transaction features cached\")\n        else:\n            print(\"‚ôªÔ∏è  Reusing cached transaction features\")\n        \n        return self.cache_stages['transaction_features']\n    \n    def analyze_segment_behavior(self):\n        \"\"\"Analyze behavior by customer segments\"\"\"\n        print(\"\\nüìä Segment Behavior Analysis\")\n        \n        features = self.get_transaction_features()\n        \n        result = features.groupBy(\"spending_segment\", \"category\") \\\n            .agg(\n                spark_sum(\"amount\").alias(\"total_sales\"),\n                spark_count(\"*\").alias(\"transaction_count\"),\n                spark_avg(\"amount\").alias(\"avg_amount\")\n            )\n        \n        return result\n    \n    def analyze_geographic_patterns(self):\n        \"\"\"Analyze geographic spending patterns\"\"\"\n        print(\"\\nüó∫Ô∏è  Geographic Pattern Analysis\")\n        \n        features = self.get_transaction_features()\n        \n        result = features.groupBy(\"state\", \"age_group\") \\\n            .agg(\n                spark_sum(\"amount\").alias(\"total_spending\"),\n                spark_avg(\"amount\").alias(\"avg_transaction\"),\n                spark_count(\"*\").alias(\"transaction_volume\")\n            )\n        \n        return result\n    \n    def analyze_payment_preferences(self):\n        \"\"\"Analyze payment method preferences by segment\"\"\"\n        print(\"\\nüí≥ Payment Preference Analysis\")\n        \n        features = self.get_transaction_features()\n        \n        result = features.groupBy(\"spending_segment\", \"payment_method\") \\\n            .agg(\n                spark_count(\"*\").alias(\"usage_count\"),\n                spark_sum(\"amount\").alias(\"total_value\"),\n                spark_avg(\"amount\").alias(\"avg_transaction_value\")\n            )\n        \n        return result\n    \n    def run_full_analysis(self):\n        \"\"\"Run complete analysis suite with timing\"\"\"\n        print(\"üöÄ Running Full Customer Insights Analysis\")\n        print(\"=\" * 50)\n        \n        analyses = [\n            (\"Segment Behavior\", self.analyze_segment_behavior),\n            (\"Geographic Patterns\", self.analyze_geographic_patterns),\n            (\"Payment Preferences\", self.analyze_payment_preferences)\n        ]\n        \n        results = {}\n        total_time = 0\n        \n        for analysis_name, analysis_func in analyses:\n            start_time = time.time()\n            result_df = analysis_func()\n            result_count = result_df.count()\n            execution_time = time.time() - start_time\n            \n            results[analysis_name] = {\n                'count': result_count,\n                'time': execution_time,\n                'sample': result_df.take(3)\n            }\n            total_time += execution_time\n            \n            print(f\"{analysis_name}: {result_count} results in {execution_time:.4f}s\")\n        \n        print(f\"\\nüìä Total Analysis Time: {total_time:.4f}s\")\n        return results\n    \n    def cleanup(self):\n        \"\"\"Clean up all cached stages\"\"\"\n        for stage_name, stage_df in self.cache_stages.items():\n            stage_df.unpersist()\n        print(f\"üßπ Cleaned up {len(self.cache_stages)} cached stages\")\n\n# Test the advanced pipeline\nprint(\"üß™ Testing Advanced Analytics Pipeline\")\nadvanced_pipeline = CustomerInsightsPipeline()\n\n# Run the full analysis\nanalysis_results = advanced_pipeline.run_full_analysis()\n\n# Show sample results\nprint(\"\\nüìã Sample Results:\")\nfor analysis_name, metrics in analysis_results.items():\n    print(f\"\\n{analysis_name}:\")\n    print(f\"  Records: {metrics['count']}\")\n    print(f\"  Time: {metrics['time']:.4f}s\")\n    if metrics['sample']:\n        print(f\"  Sample: {metrics['sample'][0]}\")\n\n# Validation\nassert len(analysis_results) == 3, \"Should have 3 analysis results\"\nfor result in analysis_results.values():\n    assert result['count'] > 0, \"Each analysis should have results\"\n\nprint(\"\\n‚úÖ Exercise 4.1 completed! Advanced pipeline with smart caching.\")\n\n# Cleanup\nadvanced_pipeline.cleanup()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "You've mastered Spark's lazy evaluation system and optimization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Concepts Mastered:\n",
    "\n",
    "1. **DAG Construction**: How Spark builds execution graphs from transformations\n",
    "2. **Catalyst Optimizer**: Automatic query optimization including:\n",
    "   - Predicate pushdown (filters before joins)\n",
    "   - Column pruning (only needed columns)\n",
    "   - Constant folding and expression optimization\n",
    "3. **Job Stages**: Understanding narrow vs wide transformations\n",
    "4. **Shuffle Operations**: Identifying and minimizing expensive data movement\n",
    "5. **Smart Caching**: Strategic persistence for pipeline reuse\n",
    "\n",
    "### üöÄ Performance Optimization Strategies:\n",
    "\n",
    "| **Strategy** | **Technique** | **Benefit** |\n",
    "|--------------|---------------|-------------|\n",
    "| **Filter Early** | Apply filters before joins/aggregations | Reduce data size |\n",
    "| **Cache Strategically** | Persist RDDs used multiple times | Avoid recomputation |\n",
    "| **Minimize Shuffles** | Use appropriate join/aggregation patterns | Reduce network I/O |\n",
    "| **Column Pruning** | Select only needed columns | Reduce memory usage |\n",
    "| **Predicate Pushdown** | Let Catalyst optimize filter placement | Automatic optimization |\n",
    "\n",
    "### üí° Best Practices for Production:\n",
    "\n",
    "- **Monitor Spark UI**: Watch for shuffle operations and skewed partitions\n",
    "- **Use DataFrame API**: Benefit from Catalyst optimizer vs raw RDDs\n",
    "- **Partition Wisely**: Choose appropriate partition keys for joins\n",
    "- **Cache Judiciously**: Don't over-cache, clean up when done\n",
    "- **Test Query Plans**: Use `explain()` to understand execution\n",
    "- **Profile Performance**: Measure execution times and optimize bottlenecks\n",
    "\n",
    "### üîç Debugging Lazy Evaluation Issues:\n",
    "\n",
    "1. **Use `explain()`** to understand query plans\n",
    "2. **Check Spark UI** for job stages and task distribution\n",
    "3. **Monitor cache usage** and hit rates\n",
    "4. **Identify shuffle operations** (look for \"Exchange\" in plans)\n",
    "5. **Profile with different approaches** to find optimal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "print(\"üìä Lab 3 Performance Summary\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ Mastered DAG construction and analysis\")\n",
    "print(\"‚úÖ Explored Catalyst optimizer optimizations\")\n",
    "print(\"‚úÖ Analyzed job stages and shuffle operations\")\n",
    "print(\"‚úÖ Implemented advanced caching strategies\")\n",
    "print(\"‚úÖ Built reusable analytics pipelines\")\n",
    "\n",
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"\\nüéâ Lab 3: Lazy Evaluation completed successfully!\")\n",
    "print(\"üî• You're now a Spark optimization expert!\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Lab 4 - DataFrame API Introduction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}