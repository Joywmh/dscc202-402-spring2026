{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 5: Spark SQL Basics - Solutions\n",
    "\n",
    "**Objective**: Master Spark SQL for querying structured data with familiar SQL syntax.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Write complex SQL queries in Spark\n",
    "- Understand Spark SQL execution engine\n",
    "- Work with multiple data formats (CSV, Parquet, JSON)\n",
    "- Create and manage temporary views\n",
    "- Optimize SQL queries for performance\n",
    "\n",
    "**Estimated Time**: 50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nimport pandas as pd\nimport time\nimport tempfile\nimport os\nimport shutil\n\nspark = SparkSession.builder \\\n    .appName(\"Lab5-Spark-SQL-Solutions\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.logLevel\", \"ERROR\") \\\n    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .getOrCreate()\n\nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")  # Suppress warnings for cleaner output\n# Also set log level for root logger to be extra sure\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nprint(f\"üöÄ Spark SQL Lab Solutions - Version {spark.version}\")\n\n# Enhanced Spark UI URL display\nui_url = spark.sparkContext.uiWebUrl\nprint(f\"Spark UI: {ui_url}\")\nprint(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: SQL Fundamentals in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple datasets\n",
    "customers_df = spark.read.csv(\"../Datasets/customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"../Datasets/customer_transactions.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(\"../Datasets/product_catalog.csv\", header=True, inferSchema=True)\n",
    "iot_df = spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n",
    "\n",
    "# Create temporary views\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "iot_df.createOrReplaceTempView(\"iot_sensors\")\n",
    "\n",
    "print(\"üìä Datasets registered as SQL views:\")\n",
    "print(f\"  - customers: {customers_df.count():,} records\")\n",
    "print(f\"  - transactions: {transactions_df.count():,} records\")\n",
    "print(f\"  - products: {products_df.count():,} records\")\n",
    "print(f\"  - iot_sensors: {iot_df.count():,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Write SQL queries for business analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Business Intelligence Queries\n",
    "\n",
    "# Query 1: Monthly sales trends\n",
    "monthly_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_FORMAT(transaction_date, 'yyyy-MM') as year_month,\n",
    "        SUM(amount) as total_sales,\n",
    "        COUNT(*) as transaction_count,\n",
    "        AVG(amount) as avg_transaction_amount,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers\n",
    "    FROM transactions\n",
    "    GROUP BY DATE_FORMAT(transaction_date, 'yyyy-MM')\n",
    "    ORDER BY year_month\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìà Monthly Sales Trends:\")\n",
    "monthly_sales.show()\n",
    "\n",
    "# Query 2: Customer lifetime value by state\n",
    "customer_ltv = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.state,\n",
    "        COUNT(DISTINCT c.customer_id) as customer_count,\n",
    "        SUM(t.amount) as total_revenue,\n",
    "        AVG(customer_total.total_spent) as avg_customer_ltv,\n",
    "        PERCENTILE_APPROX(customer_total.total_spent, 0.5) as median_ltv,\n",
    "        MAX(customer_total.total_spent) as max_ltv\n",
    "    FROM customers c\n",
    "    JOIN transactions t ON c.customer_id = t.customer_id\n",
    "    JOIN (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            SUM(amount) as total_spent\n",
    "        FROM transactions\n",
    "        GROUP BY customer_id\n",
    "    ) customer_total ON c.customer_id = customer_total.customer_id\n",
    "    GROUP BY c.state\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí∞ Customer LTV by State:\")\n",
    "customer_ltv.show()\n",
    "\n",
    "# Query 3: Product performance analysis\n",
    "product_performance = spark.sql(\"\"\"\n",
    "    WITH category_sales AS (\n",
    "        SELECT \n",
    "            t.category,\n",
    "            SUM(t.amount) as total_sales,\n",
    "            COUNT(*) as transaction_count,\n",
    "            AVG(t.amount) as avg_transaction,\n",
    "            COUNT(DISTINCT t.customer_id) as unique_customers\n",
    "        FROM transactions t\n",
    "        GROUP BY t.category\n",
    "    ),\n",
    "    product_stats AS (\n",
    "        SELECT \n",
    "            p.category,\n",
    "            COUNT(*) as product_count,\n",
    "            AVG(p.price) as avg_product_price,\n",
    "            SUM(p.stock_quantity) as total_inventory\n",
    "        FROM products p\n",
    "        GROUP BY p.category\n",
    "    )\n",
    "    SELECT \n",
    "        cs.*,\n",
    "        ps.product_count,\n",
    "        ps.avg_product_price,\n",
    "        ps.total_inventory,\n",
    "        ROUND(cs.total_sales / cs.transaction_count, 2) as sales_per_transaction,\n",
    "        ROUND(cs.total_sales / cs.unique_customers, 2) as sales_per_customer\n",
    "    FROM category_sales cs\n",
    "    LEFT JOIN product_stats ps ON cs.category = ps.category\n",
    "    ORDER BY cs.total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüõçÔ∏è Product Performance Analysis:\")\n",
    "product_performance.show()\n",
    "\n",
    "# Validation\n",
    "monthly_count = monthly_sales.count()\n",
    "state_count = customer_ltv.count()\n",
    "product_count = product_performance.count()\n",
    "\n",
    "assert monthly_count > 0, \"Should have monthly sales data\"\n",
    "assert state_count > 0, \"Should have state-level LTV data\"\n",
    "assert product_count > 0, \"Should have product performance data\"\n",
    "\n",
    "print(f\"\\n‚úì Exercise 1.1 completed!\")\n",
    "print(f\"üìä Generated {monthly_count} monthly reports, {state_count} state analyses, {product_count} category insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Advanced SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Window functions in SQL\ncustomer_rankings = spark.sql(\"\"\"\n    WITH customer_totals AS (\n        SELECT \n            c.customer_id,\n            c.name,\n            c.state,\n            SUM(t.amount) as total_spent,\n            COUNT(t.transaction_id) as transaction_count\n        FROM customers c\n        JOIN transactions t ON c.customer_id = t.customer_id\n        GROUP BY c.customer_id, c.name, c.state\n    ),\n    ranked_customers AS (\n        SELECT \n            *,\n            RANK() OVER (PARTITION BY state ORDER BY total_spent DESC) as state_rank,\n            PERCENT_RANK() OVER (ORDER BY total_spent DESC) as percentile_rank\n        FROM customer_totals\n    )\n    SELECT *\n    FROM ranked_customers\n    WHERE state_rank <= 3\n    ORDER BY state, state_rank\n\"\"\")\n\nprint(\"üèÜ Top customers by state:\")\ncustomer_rankings.show(20)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Complex analytical queries with CTEs and window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Advanced Analytics Challenge\n\n# Challenge 1: Customer Cohort Analysis\ncohort_analysis = spark.sql(\"\"\"\n    WITH customer_cohorts AS (\n        SELECT \n            customer_id,\n            DATE_FORMAT(signup_date, 'yyyy-MM') as signup_cohort,\n            signup_date\n        FROM customers\n    ),\n    transaction_months AS (\n        SELECT \n            t.customer_id,\n            t.transaction_date,\n            DATE_FORMAT(t.transaction_date, 'yyyy-MM') as transaction_month,\n            cc.signup_cohort,\n            cc.signup_date\n        FROM transactions t\n        JOIN customer_cohorts cc ON t.customer_id = cc.customer_id\n    ),\n    cohort_data AS (\n        SELECT \n            signup_cohort,\n            transaction_month,\n            MONTHS_BETWEEN(TO_DATE(transaction_month, 'yyyy-MM'), TO_DATE(signup_cohort, 'yyyy-MM')) as months_since_signup,\n            COUNT(DISTINCT customer_id) as active_customers,\n            SUM(1) as transactions,\n            SUM(amount) as revenue\n        FROM (\n            SELECT \n                tm.*,\n                1 as amount  -- Simplified for demo\n            FROM transaction_months tm\n        ) tm_with_amount\n        GROUP BY signup_cohort, transaction_month, MONTHS_BETWEEN(TO_DATE(transaction_month, 'yyyy-MM'), TO_DATE(signup_cohort, 'yyyy-MM'))\n    ),\n    cohort_sizes AS (\n        SELECT \n            signup_cohort,\n            COUNT(DISTINCT customer_id) as cohort_size\n        FROM customer_cohorts\n        GROUP BY signup_cohort\n    )\n    SELECT \n        cd.signup_cohort,\n        cs.cohort_size,\n        cd.months_since_signup,\n        cd.active_customers,\n        ROUND(cd.active_customers * 100.0 / cs.cohort_size, 2) as retention_rate\n    FROM cohort_data cd\n    JOIN cohort_sizes cs ON cd.signup_cohort = cs.signup_cohort\n    WHERE cd.months_since_signup >= 0 AND cd.months_since_signup <= 12\n    ORDER BY cd.signup_cohort, cd.months_since_signup\n\"\"\")\n\nprint(\"üìä Customer Cohort Analysis:\")\ncohort_analysis.show(30)\n\n# Challenge 2: IoT Sensor Analytics (using actual available columns)\nsensor_insights = spark.sql(\"\"\"\n    WITH daily_sensor_stats AS (\n        SELECT \n            DATE(timestamp) as reading_date,\n            location,\n            status,\n            AVG(value) as avg_value,\n            MIN(value) as min_value,\n            MAX(value) as max_value,\n            STDDEV(value) as value_stddev,\n            COUNT(*) as reading_count,\n            AVG(battery_level) as avg_battery,\n            AVG(signal_strength) as avg_signal\n        FROM iot_sensors\n        GROUP BY DATE(timestamp), location, status\n    ),\n    sensor_stats_with_thresholds AS (\n        SELECT \n            *,\n            AVG(avg_value) OVER (PARTITION BY location, status) as overall_avg_value,\n            STDDEV(avg_value) OVER (PARTITION BY location, status) as overall_value_stddev\n        FROM daily_sensor_stats\n    )\n    SELECT \n        reading_date,\n        location,\n        status,\n        ROUND(avg_value, 2) as avg_sensor_value,\n        ROUND(min_value, 2) as min_sensor_value,\n        ROUND(max_value, 2) as max_sensor_value,\n        reading_count,\n        ROUND(avg_battery, 2) as avg_battery_level,\n        ROUND(avg_signal, 2) as avg_signal_strength,\n        CASE \n            WHEN ABS(avg_value - overall_avg_value) > 2 * overall_value_stddev THEN 'ANOMALY'\n            WHEN ABS(avg_value - overall_avg_value) > overall_value_stddev THEN 'WARNING'\n            ELSE 'NORMAL'\n        END as health_status\n    FROM sensor_stats_with_thresholds\n    ORDER BY reading_date DESC, location, status\n\"\"\")\n\nprint(\"\\nüîß IoT Sensor Insights:\")\nsensor_insights.show(20)\n\n# Challenge 3: Cross-dataset correlation (simplified to use available data)\nsensor_location_correlation = spark.sql(\"\"\"\n    WITH daily_sensor_summary AS (\n        SELECT \n            DATE(timestamp) as sensor_date,\n            location,\n            AVG(value) as avg_sensor_value,\n            COUNT(*) as sensor_readings,\n            CASE \n                WHEN AVG(value) > 75 THEN 'High'\n                WHEN AVG(value) > 25 THEN 'Medium'\n                ELSE 'Low'\n            END as sensor_category\n        FROM iot_sensors\n        WHERE status = 'active'\n        GROUP BY DATE(timestamp), location\n    ),\n    daily_sales AS (\n        SELECT \n            DATE(transaction_date) as sales_date,\n            SUM(amount) as daily_sales,\n            COUNT(*) as transaction_count\n        FROM transactions\n        GROUP BY DATE(transaction_date)\n    )\n    SELECT \n        dss.sensor_category,\n        COUNT(*) as days_count,\n        AVG(ds.daily_sales) as avg_daily_sales,\n        AVG(ds.transaction_count) as avg_transactions,\n        AVG(dss.avg_sensor_value) as avg_sensor_reading\n    FROM daily_sensor_summary dss\n    JOIN daily_sales ds ON dss.sensor_date = ds.sales_date\n    GROUP BY dss.sensor_category\n    ORDER BY avg_daily_sales DESC\n\"\"\")\n\nprint(\"\\nüì° Sensor Activity vs Sales Correlation:\")\nsensor_location_correlation.show()\n\n# Validation\ncohort_count = cohort_analysis.count()\nsensor_count = sensor_insights.count()\ncorrelation_count = sensor_location_correlation.count()\n\nassert cohort_count > 0, \"Should have cohort analysis data\"\nassert sensor_count >= 0, \"Should have sensor insights (may be 0 if no data)\"\nassert correlation_count >= 0, \"Should have sensor-sales correlation (may be 0 if no matching data)\"\n\nprint(f\"\\n‚úì Exercise 2.1 completed!\")\nprint(f\"üìà Generated {cohort_count} cohort points, {sensor_count} sensor insights, {correlation_count} correlation patterns\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 3: Performance Optimization and Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query performance across different optimizations\n",
    "print(\"‚ö° Query Performance Analysis\")\n",
    "\n",
    "# Baseline query\n",
    "baseline_query = \"\"\"\n",
    "    SELECT c.state, t.category, SUM(t.amount) as total_sales\n",
    "    FROM customers c\n",
    "    JOIN transactions t ON c.customer_id = t.customer_id\n",
    "    GROUP BY c.state, t.category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "# Optimized query with filtering\n",
    "optimized_query = \"\"\"\n",
    "    SELECT c.state, t.category, SUM(t.amount) as total_sales\n",
    "    FROM customers c\n",
    "    JOIN transactions t ON c.customer_id = t.customer_id\n",
    "    WHERE t.amount > 50 AND c.age BETWEEN 25 AND 65\n",
    "    GROUP BY c.state, t.category\n",
    "    HAVING SUM(t.amount) > 1000\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "# Time both queries\n",
    "queries = [(\"Baseline\", baseline_query), (\"Optimized\", optimized_query)]\n",
    "\n",
    "for name, query in queries:\n",
    "    start_time = time.time()\n",
    "    result = spark.sql(query)\n",
    "    count = result.count()\n",
    "    exec_time = time.time() - start_time\n",
    "    print(f\"{name} query: {count} results in {exec_time:.4f}s\")\n",
    "    \n",
    "    # Show execution plan\n",
    "    print(f\"{name} execution plan:\")\n",
    "    result.explain()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Data format performance comparison and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Data Format Performance Test\n\n# Create sample dataset for format comparison\nsample_data = spark.sql(\"\"\"\n    SELECT c.*, t.amount, t.category, t.transaction_date\n    FROM customers c\n    JOIN transactions t ON c.customer_id = t.customer_id\n    WHERE t.amount > 100\n\"\"\")\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"üìÅ Testing data formats in: {temp_dir}\")\n\nformats = [\n    (\"CSV\", \"csv\", {\"header\": True}),\n    (\"Parquet\", \"parquet\", {}),\n    (\"JSON\", \"json\", {})\n]\n\n# Save in different formats and measure\nformat_metrics = {}\n\nfor format_name, format_type, options in formats:\n    file_path = os.path.join(temp_dir, f\"data.{format_type}\")\n    \n    # Time the write operation\n    start_time = time.time()\n    if format_type == \"csv\":\n        sample_data.coalesce(1).write.mode(\"overwrite\").options(**options).csv(file_path)\n    elif format_type == \"parquet\":\n        sample_data.coalesce(1).write.mode(\"overwrite\").parquet(file_path)\n    elif format_type == \"json\":\n        sample_data.coalesce(1).write.mode(\"overwrite\").json(file_path)\n    write_time = time.time() - start_time\n    \n    # Get directory size\n    def get_dir_size(path):\n        total = 0\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                filepath = os.path.join(dirpath, filename)\n                total += os.path.getsize(filepath)\n        return total\n    \n    file_size = get_dir_size(file_path)\n    \n    # Time the read operation with proper schema handling\n    start_time = time.time()\n    if format_type == \"csv\":\n        # For CSV, use inferSchema to get proper types\n        read_df = spark.read.options(**options).option(\"inferSchema\", \"true\").csv(file_path)\n    elif format_type == \"parquet\":\n        read_df = spark.read.parquet(file_path)\n    elif format_type == \"json\":\n        read_df = spark.read.json(file_path)\n    \n    # Force execution with count\n    record_count = read_df.count()\n    read_time = time.time() - start_time\n    \n    format_metrics[format_name] = {\n        'write_time': write_time,\n        'read_time': read_time,\n        'file_size': file_size,\n        'record_count': record_count\n    }\n\n# Display performance comparison\nprint(\"\\nüìä Format Performance Comparison:\")\nfor format_name, metrics in format_metrics.items():\n    print(f\"{format_name}:\")\n    print(f\"  Write: {metrics['write_time']:.4f}s\")\n    print(f\"  Read: {metrics['read_time']:.4f}s\")\n    print(f\"  Size: {metrics['file_size']:,} bytes\")\n    print(f\"  Records: {metrics['record_count']:,}\")\n\n# Query performance test across formats\nprint(\"\\n‚ö° Query Performance by Format:\")\n\ntest_query = \"\"\"\n    SELECT category, \n           COUNT(*) as transaction_count,\n           AVG(CAST(amount AS DOUBLE)) as avg_amount,\n           SUM(CAST(amount AS DOUBLE)) as total_amount\n    FROM format_test_table\n    WHERE CAST(amount AS DOUBLE) > 150\n    GROUP BY category\n    ORDER BY total_amount DESC\n\"\"\"\n\nquery_times = {}\n\nfor format_name, format_type, options in formats:\n    file_path = os.path.join(temp_dir, f\"data.{format_type}\")\n    \n    # Read and register as temp table with proper schema handling\n    if format_type == \"csv\":\n        # For CSV, use inferSchema to get proper types\n        df = spark.read.options(**options).option(\"inferSchema\", \"true\").csv(file_path)\n    elif format_type == \"parquet\":\n        df = spark.read.parquet(file_path)\n    elif format_type == \"json\":\n        df = spark.read.json(file_path)\n    \n    df.createOrReplaceTempView(\"format_test_table\")\n    \n    # Time the query\n    start_time = time.time()\n    query_result = spark.sql(test_query)\n    result_count = query_result.count()\n    query_time = time.time() - start_time\n    \n    query_times[format_name] = query_time\n    print(f\"{format_name} query: {result_count} results in {query_time:.4f}s\")\n\n# Find best performing format\nbest_read = min(format_metrics.items(), key=lambda x: x[1]['read_time'])\nbest_write = min(format_metrics.items(), key=lambda x: x[1]['write_time'])\nsmallest_size = min(format_metrics.items(), key=lambda x: x[1]['file_size'])\nbest_query = min(query_times.items(), key=lambda x: x[1])\n\nprint(\"\\nüèÜ Performance Summary:\")\nprint(f\"Fastest read: {best_read[0]} ({best_read[1]['read_time']:.4f}s)\")\nprint(f\"Fastest write: {best_write[0]} ({best_write[1]['write_time']:.4f}s)\")\nprint(f\"Smallest size: {smallest_size[0]} ({smallest_size[1]['file_size']:,} bytes)\")\nprint(f\"Fastest query: {best_query[0]} ({best_query[1]:.4f}s)\")\n\n# Cleanup\nshutil.rmtree(temp_dir)\n\n# Validation\nassert len(format_metrics) == 3, \"Should test 3 formats\"\nassert all(metrics['record_count'] > 0 for metrics in format_metrics.values()), \"All formats should have records\"\nassert len(query_times) == 3, \"Should have query times for all formats\"\n\nprint(\"\\n‚úì Exercise 3.1 completed!\")\nprint(f\"üî¨ Tested {len(formats)} data formats with comprehensive performance analysis\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary: Spark SQL Mastery\n",
    "\n",
    "### Key Capabilities Mastered:\n",
    "1. **Standard SQL**: Full SQL-92 compliance with Spark extensions\n",
    "2. **Advanced Functions**: Window functions, CTEs, complex joins, analytical functions\n",
    "3. **Multiple Formats**: Optimal format selection based on use case\n",
    "4. **Performance**: Catalyst optimizer leveraging and query optimization\n",
    "5. **Integration**: Seamless DataFrame API and SQL integration\n",
    "\n",
    "### Performance Insights:\n",
    "| **Aspect** | **Best Practice** | **Performance Impact** |\n",
    "|------------|-------------------|------------------------|\n",
    "| **Data Format** | Parquet for analytics | 3-5x faster queries |\n",
    "| **Filtering** | WHERE before GROUP BY | Reduces shuffle data |\n",
    "| **Window Functions** | Partition appropriately | Minimizes data movement |\n",
    "| **CTEs** | Organize complex queries | Better readability & optimization |\n",
    "| **Joins** | Filter both sides first | Reduces join complexity |\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "- ‚úÖ Use Parquet for analytical workloads (smallest size, fastest queries)\n",
    "- ‚úÖ Apply filters early in queries to reduce data volume\n",
    "- ‚úÖ Leverage window functions for advanced analytics\n",
    "- ‚úÖ Use CTEs for complex query organization and reusability\n",
    "- ‚úÖ Monitor query execution plans with EXPLAIN\n",
    "- ‚úÖ Test different approaches for performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"üéâ Lab 5 completed! Spark SQL mastered.\")\n",
    "print(\"‚û°Ô∏è  Next: Lab 6 - DataFrame Operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}