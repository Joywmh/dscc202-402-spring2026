{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 6: DataFrame Operations - Solutions\n",
    "\n",
    "**Objective**: Master advanced DataFrame operations including joins, aggregations, and data manipulation.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Perform complex joins and unions\n",
    "- Apply advanced aggregation patterns\n",
    "- Manipulate DataFrame schemas and data types\n",
    "- Handle missing data and data quality issues\n",
    "- Optimize DataFrame operations for performance\n",
    "\n",
    "**Estimated Time**: 55 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport time\n\nspark = SparkSession.builder \\\n    .appName(\"Lab6-DataFrame-Ops\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.logLevel\", \"ERROR\") \\\n    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .getOrCreate()\n    \nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")  # Suppress warnings for cleaner output\nspark.sparkContext.setLogLevel(\"ERROR\")  # Extra safety for log suppression\n\nprint(f\"üöÄ DataFrame Operations Lab - Spark {spark.version}\")\n\n# Enhanced Spark UI URL display\nui_url = spark.sparkContext.uiWebUrl\nprint(f\"Spark UI: {ui_url}\")\nprint(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: Advanced Join Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "customers_df = spark.read.csv(\"../Datasets/customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"../Datasets/customer_transactions.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(\"../Datasets/product_catalog.csv\", header=True, inferSchema=True)\n",
    "social_users_df = spark.read.csv(\"../Datasets/social_media_users.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"üìä Datasets loaded for advanced operations\")\n",
    "print(f\"  - Customers: {customers_df.count():,} records\")\n",
    "print(f\"  - Transactions: {transactions_df.count():,} records\")\n",
    "print(f\"  - Products: {products_df.count():,} records\")\n",
    "print(f\"  - Social Users: {social_users_df.count():,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Implement complex join patterns and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Advanced Join Challenge\n\n# Challenge 1: Multiple table joins with different join types\nprint(\"üîó Complex Multi-table Join Analysis\")\n\n# Step 1: Customer transaction summary\ncustomer_summary = transactions_df.groupBy(\"customer_id\") \\\n    .agg(\n        F.sum(\"amount\").alias(\"total_spent\"),\n        F.count(\"*\").alias(\"transaction_count\"),\n        F.avg(\"amount\").alias(\"avg_transaction\"),\n        F.countDistinct(\"category\").alias(\"categories_purchased\"),\n        F.max(\"transaction_date\").alias(\"last_transaction_date\"),\n        F.min(\"transaction_date\").alias(\"first_transaction_date\")\n    )\n\nprint(f\"‚úÖ Customer summary created: {customer_summary.count():,} customers\")\n\n# Step 2: Product category insights\ncategory_products = products_df.groupBy(\"category\") \\\n    .agg(\n        F.count(\"*\").alias(\"product_count\"),\n        F.avg(\"price\").alias(\"avg_product_price\"),\n        F.sum(\"stock_quantity\").alias(\"total_inventory\"),\n        F.collect_list(\"name\").alias(\"product_names\")\n    )\n\nprint(f\"‚úÖ Category insights created: {category_products.count():,} categories\")\n\n# Step 3: Build comprehensive customer 360 profile\n# First, let's check and adapt to the actual social media data schema\nprint(\"üìã Social media dataset schema:\")\nsocial_users_df.printSchema()\n\n# Create a compatible social media dataset for joining\n# Map user_id to customer_id and use available columns\nsocial_adapted = social_users_df \\\n    .withColumnRenamed(\"user_id\", \"customer_id\") \\\n    .withColumn(\"platform\", F.lit(\"Social Platform\")) \\\n    .withColumnRenamed(\"follower_count\", \"followers_count\") \\\n    .withColumn(\"posts_count\", F.lit(0)) \\\n    .withColumn(\"engagement_rate\", \n               F.when(F.col(\"followers_count\") > 0, \n                     (F.col(\"following_count\") / F.col(\"followers_count\")) * 100)\n               .otherwise(0.0))\n\ncustomer_360 = customers_df \\\n    .join(customer_summary, \"customer_id\", \"inner\") \\\n    .join(social_adapted.select(\"customer_id\", \"platform\", \"followers_count\", \"posts_count\", \"engagement_rate\", \"verified\"), \n          \"customer_id\", \"left\") \\\n    .withColumn(\"customer_segment\",\n        F.when(F.col(\"total_spent\") > 5000, \"VIP\")\n        .when(F.col(\"total_spent\") > 2000, \"High Value\")\n        .when(F.col(\"total_spent\") > 500, \"Medium Value\")\n        .otherwise(\"Low Value\")\n    ) \\\n    .withColumn(\"days_as_customer\",\n        F.datediff(F.col(\"last_transaction_date\"), F.col(\"first_transaction_date\"))\n    ) \\\n    .withColumn(\"has_social_presence\", \n        F.col(\"platform\").isNotNull()\n    ) \\\n    .withColumn(\"is_verified_user\",\n        F.coalesce(F.col(\"verified\"), F.lit(False))\n    )\n\nprint(\"üìà Customer 360 Profile Sample:\")\ncustomer_360.select(\n    \"customer_id\", \"name\", \"state\", \"customer_segment\", \n    \"total_spent\", \"transaction_count\", \"categories_purchased\",\n    \"has_social_presence\", \"followers_count\", \"engagement_rate\", \"is_verified_user\"\n).show(10)\n\n# Challenge 2: Self-joins for time-series analysis\nprint(\"\\nüìä Spending Growth Analysis:\")\n\n# Create monthly spending by customer\nmonthly_trends = transactions_df \\\n    .withColumn(\"year_month\", F.date_format(F.col(\"transaction_date\"), \"yyyy-MM\")) \\\n    .groupBy(\"customer_id\", \"year_month\") \\\n    .agg(\n        F.sum(\"amount\").alias(\"monthly_spending\"),\n        F.count(\"*\").alias(\"monthly_transactions\")\n    )\n\n# Self-join to compare consecutive months\nspending_growth = monthly_trends.alias(\"current\") \\\n    .join(\n        monthly_trends.alias(\"previous\"),\n        (F.col(\"current.customer_id\") == F.col(\"previous.customer_id\")) &\n        (F.add_months(F.to_date(F.col(\"previous.year_month\"), \"yyyy-MM\"), 1) ==\n         F.to_date(F.col(\"current.year_month\"), \"yyyy-MM\")),\n        \"inner\"\n    ) \\\n    .select(\n        F.col(\"current.customer_id\"),\n        F.col(\"current.year_month\"),\n        F.col(\"current.monthly_spending\"),\n        F.col(\"previous.monthly_spending\").alias(\"prev_month_spending\"),\n        ((F.col(\"current.monthly_spending\") - F.col(\"previous.monthly_spending\")) /\n         F.col(\"previous.monthly_spending\") * 100).alias(\"growth_rate\")\n    ) \\\n    .filter(F.col(\"growth_rate\") > 20)  # Show customers with >20% growth\n\nprint(\"Customers with significant month-over-month growth (>20%):\")\nspending_growth.orderBy(F.col(\"growth_rate\").desc()).show(10)\n\n# Join performance analysis\nprint(\"\\nüîß Join Performance Analysis:\")\n\n# Test broadcast join vs regular join\nstart_time = time.time()\nbroadcast_join = customers_df.join(F.broadcast(category_products), \n                                 customers_df.state == category_products.category, \"left\")\nbroadcast_count = broadcast_join.count()\nbroadcast_time = time.time() - start_time\n\nstart_time = time.time()\nregular_join = customers_df.join(category_products, \n                               customers_df.state == category_products.category, \"left\")\nregular_count = regular_join.count()\nregular_time = time.time() - start_time\n\nprint(f\"Broadcast join: {broadcast_count:,} results in {broadcast_time:.4f}s\")\nprint(f\"Regular join: {regular_count:,} results in {regular_time:.4f}s\")\n\n# Validation\ncustomer_360_count = customer_360.count()\ngrowth_customers = spending_growth.count()\n\nassert customer_360_count > 0, \"Should have customer 360 profiles\"\nassert growth_customers >= 0, \"Should have growth analysis (may be 0)\"\n\nprint(f\"\\n‚úì Exercise 1.1 completed!\")\nprint(f\"üìä Created {customer_360_count:,} customer profiles, identified {growth_customers} high-growth customers\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Aggregations and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Pivot tables and cube/rollup operations\nprint(\"üìã Advanced Aggregation Patterns\")\n\n# Pivot analysis - sales by state and category\nsales_pivot = transactions_df.join(customers_df, \"customer_id\") \\\n    .groupBy(\"state\") \\\n    .pivot(\"category\") \\\n    .agg(F.sum(\"amount\").alias(\"total_sales\"))\n\nprint(\"üîÑ Pivot Table - Sales by State and Category:\")\nsales_pivot.show()\n\n# Cube operation for multi-dimensional analysis\nsales_cube = transactions_df.join(customers_df, \"customer_id\") \\\n    .cube(\"state\", \"category\") \\\n    .agg(F.sum(\"amount\").alias(\"total_sales\"), \n         F.count(\"*\").alias(\"transaction_count\")) \\\n    .orderBy(\"state\", \"category\")\n\nprint(\"\\nüé≤ Cube Analysis - All Combinations:\")\nsales_cube.show(20)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Build sophisticated aggregation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Advanced Aggregation Challenge\n\n# Challenge 1: Multi-level rollups\nprint(\"üìÖ Time-based Rollup Analysis\")\n\n# Add time components\ntransactions_with_time = transactions_df \\\n    .withColumn(\"year\", F.year(F.col(\"transaction_date\"))) \\\n    .withColumn(\"quarter\", F.quarter(F.col(\"transaction_date\"))) \\\n    .withColumn(\"month\", F.month(F.col(\"transaction_date\")))\n\n# Create rollup by year, quarter, month\ntime_rollup = transactions_with_time \\\n    .rollup(\"year\", \"quarter\", \"month\") \\\n    .agg(\n        F.sum(\"amount\").alias(\"total_sales\"),\n        F.count(\"*\").alias(\"transaction_count\"),\n        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n        F.avg(\"amount\").alias(\"avg_transaction_amount\")\n    ) \\\n    .withColumn(\"aggregation_level\",\n        F.when(F.col(\"year\").isNull(), \"Grand Total\")\n        .when(F.col(\"quarter\").isNull(), \"Year Total\")\n        .when(F.col(\"month\").isNull(), \"Quarter Total\")\n        .otherwise(\"Month Detail\")\n    ) \\\n    .orderBy(\"year\", \"quarter\", \"month\")\n\nprint(\"Time-based rollup results:\")\ntime_rollup.show(25)\n\n# Challenge 2: Percentile and statistical aggregations\nprint(\"\\nüìä Customer Segment Statistics:\")\n\n# First, create customer segments\ncustomer_segments = customers_df \\\n    .join(transactions_df.groupBy(\"customer_id\").agg(F.sum(\"amount\").alias(\"total_spent\")), \"customer_id\") \\\n    .withColumn(\"age_group\",\n        F.when(F.col(\"age\") < 25, \"18-25\")\n        .when(F.col(\"age\") < 35, \"26-35\")\n        .when(F.col(\"age\") < 45, \"36-45\")\n        .when(F.col(\"age\") < 55, \"46-55\")\n        .otherwise(\"55+\")\n    ) \\\n    .withColumn(\"spending_tier\",\n        F.when(F.col(\"total_spent\") > 3000, \"High\")\n        .when(F.col(\"total_spent\") > 1000, \"Medium\")\n        .otherwise(\"Low\")\n    )\n\n# Calculate advanced statistics by segment\nsegment_stats = customer_segments \\\n    .groupBy(\"age_group\", \"spending_tier\") \\\n    .agg(\n        F.count(\"*\").alias(\"customer_count\"),\n        F.avg(\"total_spent\").alias(\"avg_spending\"),\n        F.stddev(\"total_spent\").alias(\"spending_stddev\"),\n        F.min(\"total_spent\").alias(\"min_spending\"),\n        F.max(\"total_spent\").alias(\"max_spending\"),\n        F.expr(\"percentile_approx(total_spent, 0.25)\").alias(\"p25_spending\"),\n        F.expr(\"percentile_approx(total_spent, 0.5)\").alias(\"median_spending\"),\n        F.expr(\"percentile_approx(total_spent, 0.75)\").alias(\"p75_spending\"),\n        F.expr(\"percentile_approx(total_spent, 0.95)\").alias(\"p95_spending\")\n    ) \\\n    .withColumn(\"coefficient_of_variation\", \n               F.col(\"spending_stddev\") / F.col(\"avg_spending\")) \\\n    .orderBy(\"age_group\", \"spending_tier\")\n\nprint(\"Statistical analysis by customer segment:\")\nsegment_stats.show()\n\n# Challenge 3: Moving averages and window aggregations\nprint(\"\\nüìà Moving Average Analysis:\")\n\n# Monthly sales by category\nmonthly_sales = transactions_df \\\n    .withColumn(\"year_month\", F.date_format(F.col(\"transaction_date\"), \"yyyy-MM\")) \\\n    .groupBy(\"category\", \"year_month\") \\\n    .agg(F.sum(\"amount\").alias(\"monthly_sales\")) \\\n    .orderBy(\"category\", \"year_month\")\n\n# Define window for moving average\nwindow_spec = Window.partitionBy(\"category\") \\\n    .orderBy(\"year_month\") \\\n    .rowsBetween(-2, 0)  # 3-month window (current + 2 previous)\n\n# Calculate moving averages\nmoving_averages = monthly_sales \\\n    .withColumn(\"moving_avg_3month\", F.avg(\"monthly_sales\").over(window_spec)) \\\n    .withColumn(\"moving_sum_3month\", F.sum(\"monthly_sales\").over(window_spec)) \\\n    .withColumn(\"sales_rank\", \n               F.rank().over(Window.partitionBy(\"category\").orderBy(F.col(\"monthly_sales\").desc()))) \\\n    .withColumn(\"sales_vs_avg\", \n               F.col(\"monthly_sales\") - F.col(\"moving_avg_3month\")) \\\n    .withColumn(\"trend_indicator\",\n        F.when(F.col(\"sales_vs_avg\") > 0, \"Above Average\")\n        .when(F.col(\"sales_vs_avg\") < 0, \"Below Average\")\n        .otherwise(\"At Average\")\n    )\n\nprint(\"Moving average analysis by category:\")\nmoving_averages.show(20)\n\n# Advanced window analysis - lag/lead functions\ntrend_analysis = monthly_sales \\\n    .withColumn(\"prev_month_sales\", \n               F.lag(\"monthly_sales\").over(Window.partitionBy(\"category\").orderBy(\"year_month\"))) \\\n    .withColumn(\"next_month_sales\", \n               F.lead(\"monthly_sales\").over(Window.partitionBy(\"category\").orderBy(\"year_month\"))) \\\n    .withColumn(\"month_over_month_change\",\n               (F.col(\"monthly_sales\") - F.col(\"prev_month_sales\")) / F.col(\"prev_month_sales\") * 100) \\\n    .withColumn(\"trend_direction\",\n        F.when(F.col(\"month_over_month_change\") > 10, \"Strong Growth\")\n        .when(F.col(\"month_over_month_change\") > 0, \"Growth\")\n        .when(F.col(\"month_over_month_change\") > -10, \"Stable\")\n        .otherwise(\"Decline\")\n    )\n\nprint(\"\\nüìä Trend Analysis Results:\")\ntrend_analysis.filter(F.col(\"prev_month_sales\").isNotNull()).show(15)\n\n# Validation\nrollup_count = time_rollup.count()\nsegments_count = segment_stats.count()\nmoving_avg_count = moving_averages.count()\n\nassert rollup_count > 0, \"Should have rollup results\"\nassert segments_count > 0, \"Should have segment statistics\"\nassert moving_avg_count > 0, \"Should have moving average data\"\n\nprint(f\"\\n‚úì Exercise 2.1 completed!\")\nprint(f\"üìà Generated {rollup_count} rollup aggregations, {segments_count} segment analyses, {moving_avg_count} trend points\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 3: Data Quality and Schema Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Data quality assessment and cleanup\nprint(\"üîç Data Quality Assessment\")\n\n# Check for missing values\ndef analyze_missing_data(df, df_name):\n    print(f\"\\n{df_name} Missing Data Analysis:\")\n    total_rows = df.count()\n    \n    for col_name in df.columns:\n        # Get the data type of the column\n        col_type = dict(df.dtypes)[col_name]\n        \n        # For string columns, check for both null and empty string\n        if col_type in ('string', 'StringType'):\n            null_count = df.filter(F.col(col_name).isNull() | (F.col(col_name) == \"\")).count()\n        else:\n            # For non-string columns (numeric, date, etc.), only check for null\n            null_count = df.filter(F.col(col_name).isNull()).count()\n            \n        null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n        print(f\"  {col_name}: {null_count} nulls ({null_pct:.1f}%)\")\n    \n    return total_rows\n\n# Analyze all datasets\ndatasets = [\n    (customers_df, \"Customers\"),\n    (transactions_df, \"Transactions\"),\n    (products_df, \"Products\"),\n    (social_users_df, \"Social Users\")\n]\n\ntotal_records = 0\nfor df, name in datasets:\n    total_records += analyze_missing_data(df, name)\n\nprint(f\"\\nüìä Total records analyzed: {total_records:,}\")\n\n# Data type conversions and schema modifications\nprint(\"\\nüîß Schema Operations\")\n\n# Add derived columns with proper data types\nenhanced_transactions = transactions_df \\\n    .withColumn(\"amount_category\", \n        F.when(F.col(\"amount\") < 50, \"Low\")\n        .when(F.col(\"amount\") < 200, \"Medium\")\n        .otherwise(\"High\")) \\\n    .withColumn(\"transaction_month\", F.date_format(F.col(\"transaction_date\"), \"yyyy-MM\")) \\\n    .withColumn(\"is_weekend_bool\", \n        F.when(F.col(\"is_weekend\") == \"true\", True)\n        .when(F.col(\"is_weekend\") == \"false\", False)\n        .otherwise(None).cast(BooleanType())) \\\n    .withColumn(\"amount_usd\", F.round(F.col(\"amount\"), 2)) \\\n    .withColumn(\"transaction_hour\", F.hour(F.col(\"transaction_date\"))) \\\n    .withColumn(\"days_since_transaction\", \n               F.datediff(F.current_date(), F.col(\"transaction_date\")))\n\nprint(\"Enhanced schema:\")\nenhanced_transactions.printSchema()\n\nprint(\"\\nüìã Enhanced transactions sample:\")\nenhanced_transactions.select(\n    \"customer_id\", \"amount\", \"amount_category\", \"transaction_month\", \n    \"is_weekend_bool\", \"transaction_hour\", \"days_since_transaction\"\n).show(5)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Implement comprehensive data quality pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Data Quality Pipeline Challenge\n\nclass DataQualityPipeline:\n    def __init__(self, spark_session):\n        self.spark = spark_session\n        self.quality_reports = []\n    \n    def validate_schema(self, df, expected_schema):\n        \"\"\"Validate DataFrame against expected schema\"\"\"\n        actual_schema = df.schema\n        issues = []\n        \n        # Check for missing columns\n        expected_cols = {field.name: field.dataType for field in expected_schema.fields}\n        actual_cols = {field.name: field.dataType for field in actual_schema.fields}\n        \n        missing_cols = set(expected_cols.keys()) - set(actual_cols.keys())\n        extra_cols = set(actual_cols.keys()) - set(expected_cols.keys())\n        \n        if missing_cols:\n            issues.append(f\"Missing columns: {missing_cols}\")\n        \n        if extra_cols:\n            issues.append(f\"Extra columns: {extra_cols}\")\n        \n        # Check data types for common columns\n        for col_name in set(expected_cols.keys()) & set(actual_cols.keys()):\n            if str(expected_cols[col_name]) != str(actual_cols[col_name]):\n                issues.append(f\"Type mismatch for {col_name}: expected {expected_cols[col_name]}, got {actual_cols[col_name]}\")\n        \n        return issues\n    \n    def detect_outliers(self, df, column, method='iqr'):\n        \"\"\"Detect outliers in numeric columns\"\"\"\n        if method == 'iqr':\n            # Calculate IQR-based outliers\n            quartiles = df.select(\n                F.expr(f\"percentile_approx({column}, 0.25)\").alias(\"q1\"),\n                F.expr(f\"percentile_approx({column}, 0.75)\").alias(\"q3\")\n            ).collect()[0]\n            \n            q1, q3 = quartiles['q1'], quartiles['q3']\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n            \n            outliers_df = df.withColumn(\"is_outlier\", \n                (F.col(column) < lower_bound) | (F.col(column) > upper_bound))\n            \n        elif method == 'zscore':\n            # Calculate Z-score based outliers\n            stats = df.select(F.avg(column).alias(\"mean\"), F.stddev(column).alias(\"stddev\")).collect()[0]\n            mean_val, stddev_val = stats['mean'], stats['stddev']\n            \n            outliers_df = df.withColumn(\"z_score\", \n                F.abs(F.col(column) - F.lit(mean_val)) / F.lit(stddev_val)) \\\n                .withColumn(\"is_outlier\", F.col(\"z_score\") > 3)\n        \n        return outliers_df\n    \n    def standardize_formats(self, df):\n        \"\"\"Standardize data formats\"\"\"\n        standardized = df\n        \n        # Clean email format if email column exists\n        if \"email\" in df.columns:\n            standardized = standardized.withColumn(\"email_clean\", \n                F.lower(F.trim(F.col(\"email\"))))\n        \n        # Standardize name format if name column exists\n        if \"name\" in df.columns:\n            standardized = standardized.withColumn(\"name_standardized\", \n                F.initcap(F.trim(F.col(\"name\"))))\n        \n        # Standardize phone format if phone column exists\n        if \"phone\" in df.columns:\n            standardized = standardized.withColumn(\"phone_clean\",\n                F.regexp_replace(F.col(\"phone\"), \"[^0-9]\", \"\"))\n        \n        # Standardize state codes\n        if \"state\" in df.columns:\n            standardized = standardized.withColumn(\"state_code\", \n                F.upper(F.trim(F.col(\"state\"))))\n        \n        return standardized\n    \n    def handle_missing_values(self, df, strategy='smart'):\n        \"\"\"Handle missing values with different strategies\"\"\"\n        if strategy == 'smart':\n            # Smart imputation logic based on column types and business logic\n            cleaned = df\n            \n            # Numeric columns - use median\n            numeric_cols = [field.name for field in df.schema.fields \n                          if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))]\n            \n            for col_name in numeric_cols:\n                if col_name in ['age', 'amount']:\n                    median_val = df.select(F.expr(f\"percentile_approx({col_name}, 0.5)\")).collect()[0][0]\n                    cleaned = cleaned.fillna({col_name: median_val})\n            \n            # Categorical columns - use mode or default values\n            categorical_mappings = {\n                'state': 'Unknown',\n                'category': 'General',\n                'payment_method': 'Unknown',\n                'gender': 'Unknown'\n            }\n            \n            for col_name, default_val in categorical_mappings.items():\n                if col_name in df.columns:\n                    cleaned = cleaned.fillna({col_name: default_val})\n            \n        else:\n            # Simple strategies\n            if strategy == 'drop':\n                cleaned = df.dropna()\n            elif strategy == 'zero':\n                cleaned = df.fillna(0)\n            else:\n                cleaned = df.fillna('Unknown')\n        \n        return cleaned\n    \n    def validate_business_rules(self, df):\n        \"\"\"Validate business logic constraints\"\"\"\n        violations = []\n        \n        # Rule 1: Negative amounts\n        if \"amount\" in df.columns:\n            negative_amounts = df.filter(F.col(\"amount\") < 0)\n            violations.append((\"Negative amounts\", negative_amounts.count()))\n        \n        # Rule 2: Invalid ages\n        if \"age\" in df.columns:\n            invalid_ages = df.filter((F.col(\"age\") < 18) | (F.col(\"age\") > 120))\n            violations.append((\"Invalid ages\", invalid_ages.count()))\n        \n        # Rule 3: Future transaction dates\n        if \"transaction_date\" in df.columns:\n            future_dates = df.filter(F.col(\"transaction_date\") > F.current_date())\n            violations.append((\"Future dates\", future_dates.count()))\n        \n        # Rule 4: Invalid email formats\n        if \"email\" in df.columns:\n            invalid_emails = df.filter(~F.col(\"email\").rlike(r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"))\n            violations.append((\"Invalid emails\", invalid_emails.count()))\n        \n        total_violations = sum(count for _, count in violations)\n        print(f\"‚ö†Ô∏è  Found {total_violations} total business rule violations:\")\n        for rule, count in violations:\n            if count > 0:\n                print(f\"  - {rule}: {count} violations\")\n        \n        return violations\n    \n    def generate_quality_report(self, df, df_name):\n        \"\"\"Generate comprehensive quality report\"\"\"\n        total_rows = df.count()\n        total_columns = len(df.columns)\n        \n        # Calculate missing value percentages with type-safe null checking\n        null_counts = {}\n        for col_name in df.columns:\n            # Get the data type of the column\n            col_type = dict(df.dtypes)[col_name]\n            \n            # For string columns, check for both null and empty string\n            if col_type in ('string', 'StringType'):\n                null_count = df.filter(F.col(col_name).isNull() | (F.col(col_name) == \"\")).count()\n            else:\n                # For non-string columns (numeric, date, etc.), only check for null\n                null_count = df.filter(F.col(col_name).isNull()).count()\n                \n            null_counts[col_name] = (null_count, (null_count / total_rows) * 100)\n        \n        # Duplicate analysis\n        if \"customer_id\" in df.columns:\n            duplicates = df.count() - df.dropDuplicates([\"customer_id\"]).count()\n        else:\n            duplicates = df.count() - df.dropDuplicates().count()\n        \n        report = {\n            'dataset': df_name,\n            'total_rows': total_rows,\n            'total_columns': total_columns,\n            'null_analysis': null_counts,\n            'duplicates': duplicates,\n            'completeness_score': (1 - sum(count for count, _ in null_counts.values()) / \n                                 (total_rows * total_columns)) * 100\n        }\n        \n        self.quality_reports.append(report)\n        return report\n\n# Test the data quality pipeline\nprint(\"üî¨ Testing Data Quality Pipeline\")\n\ndq_pipeline = DataQualityPipeline(spark)\n\n# 1. Schema Validation\nprint(\"\\n1. Schema Validation:\")\nexpected_customer_schema = StructType([\n    StructField(\"customer_id\", StringType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"email\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"gender\", StringType(), True),\n    StructField(\"state\", StringType(), True),\n    StructField(\"signup_date\", StringType(), True)\n])\n\nschema_issues = dq_pipeline.validate_schema(customers_df, expected_customer_schema)\nif schema_issues:\n    print(\"Schema validation issues found:\")\n    for issue in schema_issues:\n        print(f\"  - {issue}\")\nelse:\n    print(\"‚úÖ Schema validation passed\")\n\n# 2. Missing Value Analysis and Handling\nprint(\"\\n2. Missing Value Analysis:\")\ncustomers_cleaned = dq_pipeline.handle_missing_values(customers_df, strategy='smart')\ntransactions_cleaned = dq_pipeline.handle_missing_values(transactions_df, strategy='smart')\n\nprint(f\"Customers: {customers_df.count():,} ‚Üí {customers_cleaned.count():,} after cleaning\")\nprint(f\"Transactions: {transactions_df.count():,} ‚Üí {transactions_cleaned.count():,} after cleaning\")\n\n# 3. Outlier Detection\nprint(\"\\n3. Outlier Detection:\")\ncustomers_with_outliers = dq_pipeline.detect_outliers(customers_cleaned, \"age\", method='iqr')\nage_outliers = customers_with_outliers.filter(F.col(\"is_outlier\")).count()\nprint(f\"Age outliers detected: {age_outliers} customers\")\n\ntransactions_with_outliers = dq_pipeline.detect_outliers(transactions_cleaned, \"amount\", method='iqr')\namount_outliers = transactions_with_outliers.filter(F.col(\"is_outlier\")).count()\nprint(f\"Transaction amount outliers: {amount_outliers} transactions\")\n\n# 4. Business Rule Validation\nprint(\"\\n4. Business Rule Validation:\")\ncustomer_violations = dq_pipeline.validate_business_rules(customers_cleaned)\ntransaction_violations = dq_pipeline.validate_business_rules(transactions_cleaned)\n\n# 5. Data Standardization\nprint(\"\\n5. Data Standardization:\")\ncustomers_standardized = dq_pipeline.standardize_formats(customers_cleaned)\nprint(\"Sample standardized customer data:\")\ncustomers_standardized.select(\n    \"customer_id\", \"name\", \"name_standardized\", \"email\", \"email_clean\", \"state\", \"state_code\"\n).show(3)\n\n# 6. Quality Report Generation\nprint(\"\\nüìä Quality Report Summary:\")\ncustomer_report = dq_pipeline.generate_quality_report(customers_standardized, \"Customers\")\ntransaction_report = dq_pipeline.generate_quality_report(transactions_cleaned, \"Transactions\")\n\nfor report in dq_pipeline.quality_reports:\n    print(f\"\\n{report['dataset']} Quality Report:\")\n    print(f\"  - Total Records: {report['total_rows']:,}\")\n    print(f\"  - Columns: {report['total_columns']}\")\n    print(f\"  - Completeness Score: {report['completeness_score']:.1f}%\")\n    print(f\"  - Duplicates: {report['duplicates']:,}\")\n    \n    # Show columns with highest missing values\n    high_missing = [(col, pct) for col, (count, pct) in report['null_analysis'].items() if pct > 0]\n    if high_missing:\n        print(\"  - Columns with missing values:\")\n        for col_name, pct in sorted(high_missing, key=lambda x: x[1], reverse=True)[:3]:\n            print(f\"    ‚Ä¢ {col_name}: {pct:.1f}% missing\")\n\n# Validation\nassert len(dq_pipeline.quality_reports) == 2, \"Should have 2 quality reports\"\nassert customers_standardized.count() > 0, \"Should have standardized customers\"\nassert transactions_cleaned.count() > 0, \"Should have cleaned transactions\"\n\nprint(f\"\\n‚úì Exercise 3.1 completed!\")\nprint(f\"üîç Comprehensive data quality pipeline implemented and tested\")\nprint(f\"üìä Analyzed {sum(r['total_rows'] for r in dq_pipeline.quality_reports):,} total records\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary: DataFrame Operations Mastery\n",
    "\n",
    "### Advanced Capabilities Covered:\n",
    "\n",
    "1. **Complex Joins**: Multi-table joins, self-joins, broadcast optimization\n",
    "2. **Advanced Aggregations**: Pivot, cube, rollup, percentiles, window functions\n",
    "3. **Schema Operations**: Type conversions, derived columns, validation\n",
    "4. **Data Quality**: Missing value handling, outlier detection, standardization\n",
    "5. **Performance**: Join optimization, aggregation strategies, window operations\n",
    "\n",
    "### Key Techniques Mastered:\n",
    "\n",
    "| **Operation** | **Technique** | **Use Case** |\n",
    "|---------------|---------------|-------------|\n",
    "| **Joins** | Broadcast joins for small tables | Performance optimization |\n",
    "| **Aggregations** | Pivot/Cube for multi-dimensional analysis | Business intelligence |\n",
    "| **Window Functions** | Moving averages, rankings, lag/lead | Time series analysis |\n",
    "| **Data Quality** | Outlier detection, validation rules | Data reliability |\n",
    "| **Schema Ops** | Type casting, derived columns | Data transformation |\n",
    "\n",
    "### Production-Ready Patterns:\n",
    "\n",
    "- ‚úÖ **Comprehensive data quality pipeline** with automated validation\n",
    "- ‚úÖ **Multi-dimensional aggregation** using cube and rollup operations\n",
    "- ‚úÖ **Advanced time-series analysis** with window functions\n",
    "- ‚úÖ **Statistical outlier detection** using IQR and Z-score methods\n",
    "- ‚úÖ **Business rule validation** with configurable constraints\n",
    "- ‚úÖ **Performance optimization** through strategic join ordering\n",
    "\n",
    "### Best Practices Applied:\n",
    "- Choose appropriate join types and broadcast small dimension tables\n",
    "- Apply filters before joins to reduce shuffle data\n",
    "- Use window functions efficiently with proper partitioning\n",
    "- Handle data quality issues systematically with configurable pipelines\n",
    "- Monitor and optimize DataFrame operations through execution plan analysis\n",
    "- Implement comprehensive validation and error handling patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"üéâ Lab 6 completed! Advanced DataFrame operations mastered.\")\n",
    "print(\"‚û°Ô∏è  Next: Lab 7 - User-Defined Functions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}