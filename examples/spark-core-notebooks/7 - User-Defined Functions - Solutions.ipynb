{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 7: User-Defined Functions (UDFs) - Solutions\n",
    "\n",
    "**Objective**: Master creating and optimizing User-Defined Functions in Spark for custom business logic.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Create and register UDFs for custom transformations\n",
    "- Understand UDF performance implications and optimization\n",
    "- Implement vectorized UDFs with pandas\n",
    "- Apply UDFs to real-world business scenarios\n",
    "- Debug and troubleshoot UDF issues\n",
    "\n",
    "**Estimated Time**: 50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf, pandas_udf, col, when, regexp_extract, split, lit, datediff, current_date, sum, avg, count\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport numpy as np\nimport time\nimport re\nimport json\nimport os\n\n# Fix for Codespace pandas UDF support - override environment variables\nprint(\"üîß Configuring Spark for pandas UDF support in Codespace\")\nsystem_python = \"/usr/local/python/3.11.13/bin/python3\"\nos.environ['PYSPARK_PYTHON'] = system_python\nos.environ['PYSPARK_DRIVER_PYTHON'] = system_python\n\n# Create Spark session with pandas UDF support\nspark = SparkSession.builder \\\n    .appName(\"Lab7-UDFs-Solutions\") \\\n    .config(\"spark.pyspark.python\", system_python) \\\n    .config(\"spark.pyspark.driver.python\", system_python) \\\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n    .config(\"spark.driver.memory\", \"2g\") \\\n    .config(\"spark.executor.memory\", \"1g\") \\\n    .config(\"spark.master\", \"local[2]\") \\\n    .config(\"spark.sql.adaptive.logLevel\", \"ERROR\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .getOrCreate()\n\nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")  # Suppress warnings for cleaner output\nspark.sparkContext.setLogLevel(\"ERROR\")  # Extra safety for log suppression\n\nprint(f\"üöÄ UDF Lab Solutions - Spark {spark.version} with Arrow: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\nprint(f\"‚úÖ Pandas available: {pd.__version__}\")\nprint(f\"‚úÖ Python configured: {spark.conf.get('spark.pyspark.python')}\")\n\n# Enhanced Spark UI URL display\nui_url = spark.sparkContext.uiWebUrl\nprint(f\"Spark UI: {ui_url}\")\nprint(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: Basic UDF Creation and Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "customers_df = spark.read.csv(\"../Datasets/customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"../Datasets/customer_transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"üìä Datasets loaded for UDF examples\")\n",
    "print(f\"  - Customers: {customers_df.count():,} records\")\n",
    "print(f\"  - Transactions: {transactions_df.count():,} records\")\n",
    "\n",
    "# Simple UDF examples\n",
    "def categorize_age(age):\n",
    "    \"\"\"Categorize customers by age group\"\"\"\n",
    "    if age is None:\n",
    "        return \"Unknown\"\n",
    "    elif age < 25:\n",
    "        return \"Gen Z\"\n",
    "    elif age < 40:\n",
    "        return \"Millennial\"\n",
    "    elif age < 55:\n",
    "        return \"Gen X\"\n",
    "    else:\n",
    "        return \"Boomer\"\n",
    "\n",
    "# Register as UDF\n",
    "age_category_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "customers_with_generation = customers_df.withColumn(\n",
    "    \"generation\", \n",
    "    age_category_udf(col(\"age\"))\n",
    ")\n",
    "\n",
    "print(\"üë• Customer generations:\")\n",
    "customers_with_generation.groupBy(\"generation\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Create business logic UDFs for data enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Business Logic UDF Challenge\n",
    "\n",
    "# UDF 1: Customer Risk Scoring\n",
    "def calculate_risk_score(age, total_spent, transaction_count, state):\n",
    "    \"\"\"Calculate customer risk score based on multiple factors\"\"\"\n",
    "    if age is None or total_spent is None or transaction_count is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    risk_score = 0\n",
    "    \n",
    "    # Age factor - younger and very old customers are higher risk\n",
    "    if age < 25:\n",
    "        risk_score += 5\n",
    "    elif age > 65:\n",
    "        risk_score += 3\n",
    "    \n",
    "    # Spending factor - very high or very low average spending is risky\n",
    "    avg_transaction = total_spent / max(transaction_count, 1)\n",
    "    if avg_transaction > 1000 or avg_transaction < 10:\n",
    "        risk_score += 4\n",
    "    \n",
    "    # Frequency factor - very few transactions is risky\n",
    "    if transaction_count < 3:\n",
    "        risk_score += 6\n",
    "    elif transaction_count > 50:\n",
    "        risk_score += 2  # Very high frequency also suspicious\n",
    "    \n",
    "    # Geographic factor (example high-risk states)\n",
    "    high_risk_states = ['FL', 'NV', 'CA']  # Example states with higher fraud rates\n",
    "    if state in high_risk_states:\n",
    "        risk_score += 2\n",
    "    \n",
    "    # Return risk category\n",
    "    if risk_score >= 15:\n",
    "        return \"High\"\n",
    "    elif risk_score >= 8:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# Register risk scoring UDF\n",
    "risk_score_udf = udf(calculate_risk_score, StringType())\n",
    "\n",
    "# UDF 2: Email Domain Classification\n",
    "def classify_email_domain(email):\n",
    "    \"\"\"Classify email domains into categories\"\"\"\n",
    "    if email is None or '@' not in email:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Extract domain from email\n",
    "    domain = email.split('@')[1].lower() if '@' in email else \"\"\n",
    "    \n",
    "    # Classify domains\n",
    "    business_domains = ['company.com', 'corp.com', 'business.com', 'enterprise.com']\n",
    "    consumer_domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'aol.com']\n",
    "    educational_domains = ['edu', '.edu', 'university.edu']\n",
    "    \n",
    "    if domain in business_domains or 'corp' in domain or 'company' in domain:\n",
    "        return \"Business\"\n",
    "    elif domain in consumer_domains:\n",
    "        return \"Consumer\"\n",
    "    elif any(edu in domain for edu in educational_domains):\n",
    "        return \"Educational\"\n",
    "    elif domain.endswith('.gov'):\n",
    "        return \"Government\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Register email classification UDF\n",
    "email_domain_udf = udf(classify_email_domain, StringType())\n",
    "\n",
    "# UDF 3: Transaction Anomaly Detection\n",
    "def detect_anomaly(amount, customer_avg, customer_std):\n",
    "    \"\"\"Detect if transaction is anomalous for customer\"\"\"\n",
    "    if amount is None or customer_avg is None or customer_std is None:\n",
    "        return False\n",
    "    \n",
    "    if customer_std == 0:\n",
    "        # If no variation in spending, flag very different amounts\n",
    "        return abs(amount - customer_avg) > customer_avg * 0.5\n",
    "    \n",
    "    # Calculate z-score\n",
    "    z_score = abs(amount - customer_avg) / customer_std\n",
    "    \n",
    "    # Return True if anomaly (|z-score| > 2)\n",
    "    return z_score > 2.0\n",
    "\n",
    "# Register anomaly detection UDF\n",
    "anomaly_udf = udf(detect_anomaly, BooleanType())\n",
    "\n",
    "# Apply all UDFs to create enriched dataset\n",
    "print(\"üîß Applying business logic UDFs...\")\n",
    "\n",
    "# First, prepare customer statistics\n",
    "from pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count as spark_count, stddev\n",
    "\n",
    "customer_stats = transactions_df.groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "        spark_count(\"*\").alias(\"transaction_count\"),\n",
    "        spark_avg(\"amount\").alias(\"avg_amount\"),\n",
    "        stddev(\"amount\").alias(\"std_amount\")\n",
    "    )\n",
    "\n",
    "# Apply UDFs to customers\n",
    "enriched_customers = customers_df \\\n",
    "    .join(customer_stats, \"customer_id\", \"left\") \\\n",
    "    .withColumn(\"risk_category\",\n",
    "        risk_score_udf(col(\"age\"), col(\"total_spent\"), col(\"transaction_count\"), col(\"state\"))\n",
    "    ) \\\n",
    "    .withColumn(\"email_domain_type\",\n",
    "        email_domain_udf(col(\"email\"))\n",
    "    )\n",
    "\n",
    "# Apply anomaly detection to transactions\n",
    "enriched_transactions = transactions_df \\\n",
    "    .join(customer_stats, \"customer_id\", \"left\") \\\n",
    "    .withColumn(\"is_anomaly\",\n",
    "        anomaly_udf(col(\"amount\"), col(\"avg_amount\"), col(\"std_amount\"))\n",
    "    )\n",
    "\n",
    "print(\"üìä UDF Results:\")\n",
    "\n",
    "print(\"\\nRisk Distribution:\")\n",
    "enriched_customers.groupBy(\"risk_category\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nEmail Domain Classification:\")\n",
    "enriched_customers.groupBy(\"email_domain_type\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nAnomaly Detection:\")\n",
    "anomaly_stats = enriched_transactions.groupBy(\"is_anomaly\").count()\n",
    "anomaly_stats.show()\n",
    "\n",
    "# Show some anomalous transactions\n",
    "print(\"Sample anomalous transactions:\")\n",
    "enriched_transactions.filter(col(\"is_anomaly\") == True) \\\n",
    "    .select(\"customer_id\", \"amount\", \"avg_amount\", \"std_amount\") \\\n",
    "    .show(5)\n",
    "\n",
    "# Validation\n",
    "risk_categories = enriched_customers.select(\"risk_category\").distinct().count()\n",
    "email_types = enriched_customers.select(\"email_domain_type\").distinct().count()\n",
    "anomaly_count = enriched_transactions.filter(col(\"is_anomaly\")).count()\n",
    "\n",
    "assert risk_categories > 0, \"Should have risk categories\"\n",
    "assert email_types > 0, \"Should have email domain types\"\n",
    "assert anomaly_count >= 0, \"Should have anomaly detection results\"\n",
    "\n",
    "print(f\"\\n‚úì Exercise 1.1 completed!\")\n",
    "print(f\"üìà Generated {risk_categories} risk categories, {email_types} email types, {anomaly_count} anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Performance Optimization and Pandas UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regular UDF vs Pandas UDF performance\n",
    "print(\"‚ö° UDF Performance Comparison\")\n",
    "\n",
    "# Regular UDF for complex calculation\n",
    "def calculate_loyalty_score_regular(transaction_count, total_spent, days_since_signup, avg_amount):\n",
    "    \"\"\"Calculate customer loyalty score (regular UDF)\"\"\"\n",
    "    if transaction_count is None or total_spent is None or days_since_signup is None or avg_amount is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Complex business logic\n",
    "    frequency_score = min(transaction_count / 10.0, 1.0) * 30\n",
    "    volume_score = min(total_spent / 5000.0, 1.0) * 40\n",
    "    tenure_score = min(days_since_signup / 365.0, 1.0) * 20\n",
    "    consistency_score = (1.0 - abs(avg_amount - 100) / 100.0) * 10 if avg_amount > 0 else 0\n",
    "    \n",
    "    return max(0, frequency_score + volume_score + tenure_score + consistency_score)\n",
    "\n",
    "# Pandas UDF for vectorized calculation\n",
    "@pandas_udf(returnType=DoubleType())\n",
    "def calculate_loyalty_score_pandas(transaction_count: pd.Series, total_spent: pd.Series, \n",
    "                                 days_since_signup: pd.Series, avg_amount: pd.Series) -> pd.Series:\n",
    "    \"\"\"Calculate customer loyalty score (Pandas UDF - vectorized)\"\"\"\n",
    "    # Handle nulls\n",
    "    transaction_count = transaction_count.fillna(0)\n",
    "    total_spent = total_spent.fillna(0)\n",
    "    days_since_signup = days_since_signup.fillna(0)\n",
    "    avg_amount = avg_amount.fillna(0)\n",
    "    \n",
    "    frequency_score = np.minimum(transaction_count / 10.0, 1.0) * 30\n",
    "    volume_score = np.minimum(total_spent / 5000.0, 1.0) * 40\n",
    "    tenure_score = np.minimum(days_since_signup / 365.0, 1.0) * 20\n",
    "    consistency_score = np.where(avg_amount > 0, \n",
    "                                (1.0 - np.abs(avg_amount - 100) / 100.0) * 10, 0)\n",
    "    \n",
    "    return np.maximum(0, frequency_score + volume_score + tenure_score + consistency_score)\n",
    "\n",
    "# Register UDFs\n",
    "loyalty_regular_udf = udf(calculate_loyalty_score_regular, DoubleType())\n",
    "# Pandas UDF is already decorated\n",
    "\n",
    "# Prepare test dataset with customer metrics\n",
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "customer_metrics = transactions_df.join(customers_df, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\", \"signup_date\") \\\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"transaction_count\"),\n",
    "        spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "        spark_avg(\"amount\").alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .withColumn(\"days_since_signup\", \n",
    "                datediff(current_date(), col(\"signup_date\")))\n",
    "\n",
    "print(f\"üìä Testing with {customer_metrics.count():,} customer records\")\n",
    "\n",
    "# Performance test\n",
    "print(\"\\n‚è±Ô∏è  Performance Testing:\")\n",
    "\n",
    "# Test regular UDF\n",
    "start_time = time.time()\n",
    "regular_result = customer_metrics.withColumn(\n",
    "    \"loyalty_score_regular\",\n",
    "    loyalty_regular_udf(col(\"transaction_count\"), col(\"total_spent\"), \n",
    "                       col(\"days_since_signup\"), col(\"avg_amount\"))\n",
    ").count()\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "# Test Pandas UDF\n",
    "start_time = time.time()\n",
    "pandas_result = customer_metrics.withColumn(\n",
    "    \"loyalty_score_pandas\",\n",
    "    calculate_loyalty_score_pandas(col(\"transaction_count\"), col(\"total_spent\"), \n",
    "                                  col(\"days_since_signup\"), col(\"avg_amount\"))\n",
    ").count()\n",
    "pandas_time = time.time() - start_time\n",
    "\n",
    "print(f\"Regular UDF: {regular_result:,} records in {regular_time:.4f}s\")\n",
    "print(f\"Pandas UDF: {pandas_result:,} records in {pandas_time:.4f}s\")\n",
    "\n",
    "if pandas_time > 0:\n",
    "    speedup = regular_time / pandas_time\n",
    "    print(f\"Performance gain: {speedup:.1f}x faster with Pandas UDF\")\n",
    "else:\n",
    "    print(\"Pandas UDF completed too quickly to measure accurately\")\n",
    "\n",
    "# Compare results to ensure correctness\n",
    "sample_comparison = customer_metrics.withColumn(\n",
    "    \"loyalty_regular\", loyalty_regular_udf(col(\"transaction_count\"), col(\"total_spent\"), \n",
    "                                          col(\"days_since_signup\"), col(\"avg_amount\"))\n",
    ").withColumn(\n",
    "    \"loyalty_pandas\", calculate_loyalty_score_pandas(col(\"transaction_count\"), col(\"total_spent\"), \n",
    "                                                    col(\"days_since_signup\"), col(\"avg_amount\"))\n",
    ").select(\"customer_id\", \"loyalty_regular\", \"loyalty_pandas\")\n",
    "\n",
    "print(\"\\nüîç Sample loyalty score comparison:\")\n",
    "sample_comparison.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Implement and optimize complex business calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Advanced UDF Optimization Challenge\n\n# Challenge 1: Customer Segmentation with Machine Learning Features\nprint(\"üß† Advanced Feature Engineering with UDFs\")\n\n# Import required functions with proper aliases\nfrom pyspark.sql.functions import max as spark_max, sum as spark_sum, count as spark_count, datediff, current_date\n\n# Feature 1: RFM Score (Recency, Frequency, Monetary)\n@pandas_udf(returnType=StructType([\n    StructField(\"recency_score\", IntegerType()),\n    StructField(\"frequency_score\", IntegerType()),\n    StructField(\"monetary_score\", IntegerType()),\n    StructField(\"rfm_segment\", StringType())\n]))\ndef calculate_rfm_scores(days_since_last: pd.Series, frequency: pd.Series, \n                        monetary: pd.Series) -> pd.DataFrame:\n    \"\"\"Calculate RFM scores using pandas vectorization\"\"\"\n    \n    # Handle missing values\n    days_since_last = days_since_last.fillna(365)  # Default to 1 year\n    frequency = frequency.fillna(1)\n    monetary = monetary.fillna(0)\n    \n    # Recency: 1-5 (1 = most recent, i.e., fewer days since last purchase)\n    recency_percentiles = np.percentile(days_since_last, [20, 40, 60, 80])\n    recency_score = pd.cut(days_since_last, \n                          bins=[-1] + list(recency_percentiles) + [float('inf')], \n                          labels=[5, 4, 3, 2, 1]).astype(int)\n    \n    # Frequency: 1-5 (5 = most frequent)\n    frequency_percentiles = np.percentile(frequency, [20, 40, 60, 80])\n    frequency_score = pd.cut(frequency, \n                           bins=[-1] + list(frequency_percentiles) + [float('inf')], \n                           labels=[1, 2, 3, 4, 5]).astype(int)\n    \n    # Monetary: 1-5 (5 = highest value)\n    monetary_percentiles = np.percentile(monetary, [20, 40, 60, 80])\n    monetary_score = pd.cut(monetary, \n                          bins=[-1] + list(monetary_percentiles) + [float('inf')], \n                          labels=[1, 2, 3, 4, 5]).astype(int)\n    \n    # Create segment labels based on RFM scores\n    def create_segment(r, f, m):\n        if r >= 4 and f >= 4 and m >= 4:\n            return \"Champions\"\n        elif r >= 3 and f >= 3 and m >= 3:\n            return \"Loyal Customers\"\n        elif r >= 4 and f <= 2:\n            return \"New Customers\"\n        elif r <= 2 and f >= 3:\n            return \"At Risk\"\n        elif r <= 2 and f <= 2:\n            return \"Lost Customers\"\n        else:\n            return \"Potential Loyalists\"\n    \n    rfm_segment = pd.Series([create_segment(r, f, m) \n                            for r, f, m in zip(recency_score, frequency_score, monetary_score)])\n    \n    return pd.DataFrame({\n        'recency_score': recency_score,\n        'frequency_score': frequency_score,\n        'monetary_score': monetary_score,\n        'rfm_segment': rfm_segment\n    })\n\n# Feature 2: Time-based patterns\n@pandas_udf(returnType=StructType([\n    StructField(\"seasonality_index\", DoubleType()),\n    StructField(\"trend_direction\", StringType()),\n    StructField(\"volatility_score\", DoubleType())\n]))\ndef analyze_temporal_patterns(monthly_amounts: pd.Series) -> pd.DataFrame:\n    \"\"\"Analyze customer temporal spending patterns\"\"\"\n    \n    def parse_and_analyze(amounts_str):\n        if pd.isna(amounts_str) or amounts_str == '' or amounts_str == 'null':\n            return 0.0, 'stable', 0.0\n        \n        try:\n            # Parse comma-separated amounts (simulated monthly data)\n            amounts = [float(x) for x in str(amounts_str).split(',') if x.strip()]\n            \n            if len(amounts) < 3:\n                return 0.0, 'stable', 0.0\n            \n            amounts = np.array(amounts)\n            \n            # Calculate seasonality (coefficient of variation)\n            seasonality = np.std(amounts) / np.mean(amounts) if np.mean(amounts) > 0 else 0\n            \n            # Calculate trend direction using linear regression slope\n            x = np.arange(len(amounts))\n            slope = np.polyfit(x, amounts, 1)[0] if len(amounts) > 1 else 0\n            \n            if slope > 5:\n                trend = 'increasing'\n            elif slope < -5:\n                trend = 'decreasing'\n            else:\n                trend = 'stable'\n            \n            # Calculate volatility (normalized standard deviation)\n            volatility = seasonality  # Same as seasonality for simplicity\n            \n            return seasonality, trend, volatility\n            \n        except (ValueError, TypeError):\n            return 0.0, 'stable', 0.0\n    \n    results = monthly_amounts.apply(parse_and_analyze)\n    \n    return pd.DataFrame({\n        'seasonality_index': [r[0] for r in results],\n        'trend_direction': [r[1] for r in results],\n        'volatility_score': [r[2] for r in results]\n    })\n\n# Challenge 2: Text Processing UDFs\n@pandas_udf(returnType=StructType([\n    StructField(\"sentiment_score\", DoubleType()),\n    StructField(\"entity_count\", IntegerType()),\n    StructField(\"complexity_score\", DoubleType())\n]))\ndef analyze_text_features(text_data: pd.Series) -> pd.DataFrame:\n    \"\"\"Extract features from text data (simulated for demo)\"\"\"\n    \n    def analyze_text(text):\n        if pd.isna(text) or text == '':\n            return 0.0, 0, 0.0\n        \n        text = str(text)\n        \n        # Mock sentiment analysis (positive words vs negative words)\n        positive_words = ['good', 'great', 'excellent', 'amazing', 'love', 'best']\n        negative_words = ['bad', 'terrible', 'awful', 'hate', 'worst', 'horrible']\n        \n        text_lower = text.lower()\n        pos_count = sum(1 for word in positive_words if word in text_lower)\n        neg_count = sum(1 for word in negative_words if word in text_lower)\n        \n        sentiment = (pos_count - neg_count) / max(len(text.split()), 1)\n        \n        # Mock entity count (capitalized words as potential entities)\n        entities = len([word for word in text.split() if word[0].isupper()]) if text else 0\n        \n        # Text complexity (average word length and sentence structure)\n        words = text.split()\n        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n        complexity = avg_word_length / 10.0  # Normalize to 0-1 scale\n        \n        return sentiment, entities, complexity\n    \n    results = text_data.apply(analyze_text)\n    \n    return pd.DataFrame({\n        'sentiment_score': [r[0] for r in results],\n        'entity_count': [r[1] for r in results],\n        'complexity_score': [r[2] for r in results]\n    })\n\n# Apply advanced UDFs to datasets\nprint(\"üîß Applying Advanced UDFs...\")\n\n# Prepare customer data for RFM analysis\nrfm_data = transactions_df.join(customers_df, \"customer_id\") \\\n    .groupBy(\"customer_id\") \\\n    .agg(\n        datediff(current_date(), spark_max(\"transaction_date\")).alias(\"days_since_last\"),\n        spark_count(\"*\").alias(\"frequency\"),\n        spark_sum(\"amount\").alias(\"monetary\")\n    )\n\n# Apply RFM UDF\ncustomers_with_rfm = rfm_data.withColumn(\n    \"rfm_analysis\", \n    calculate_rfm_scores(col(\"days_since_last\"), col(\"frequency\"), col(\"monetary\"))\n).select(\n    \"customer_id\",\n    col(\"rfm_analysis.recency_score\").alias(\"recency_score\"),\n    col(\"rfm_analysis.frequency_score\").alias(\"frequency_score\"),\n    col(\"rfm_analysis.monetary_score\").alias(\"monetary_score\"),\n    col(\"rfm_analysis.rfm_segment\").alias(\"rfm_segment\")\n)\n\nprint(\"üìä RFM Segmentation Results:\")\ncustomers_with_rfm.groupBy(\"rfm_segment\").count().orderBy(\"count\", ascending=False).show()\n\n# Create sample temporal data for temporal analysis\n# Simulate monthly spending patterns\nsample_temporal_data = spark.createDataFrame([\n    (\"CUST_000001\", \"100,150,120,180,200,190,210\"),\n    (\"CUST_000002\", \"50,60,55,65,70,80,90\"),\n    (\"CUST_000003\", \"200,180,160,140,120,100,80\")\n], [\"customer_id\", \"monthly_amounts\"])\n\ntemporal_analysis = sample_temporal_data.withColumn(\n    \"temporal_patterns\",\n    analyze_temporal_patterns(col(\"monthly_amounts\"))\n).select(\n    \"customer_id\",\n    col(\"temporal_patterns.seasonality_index\").alias(\"seasonality\"),\n    col(\"temporal_patterns.trend_direction\").alias(\"trend\"),\n    col(\"temporal_patterns.volatility_score\").alias(\"volatility\")\n)\n\nprint(\"\\nüìà Temporal Pattern Analysis:\")\ntemporal_analysis.show()\n\n# Performance comparison for complex operations\nprint(\"\\n‚ö° Complex UDF Performance Analysis:\")\n\n# Compare performance of RFM calculation with different approaches\nstart_time = time.time()\nrfm_pandas_count = customers_with_rfm.count()\nrfm_pandas_time = time.time() - start_time\n\nprint(f\"Pandas UDF RFM Analysis: {rfm_pandas_count:,} customers in {rfm_pandas_time:.4f}s\")\n\n# Validation\nrfm_segments = customers_with_rfm.select(\"rfm_segment\").distinct().count()\ntemporal_records = temporal_analysis.count()\n\nassert rfm_segments > 0, \"Should have RFM segments\"\nassert temporal_records > 0, \"Should have temporal analysis\"\nassert rfm_pandas_count > 0, \"Should have RFM analysis results\"\n\nprint(f\"\\n‚úì Exercise 2.1 completed!\")\nprint(f\"üß† Generated {rfm_segments} RFM segments for {rfm_pandas_count:,} customers\")\nprint(f\"üìä Analyzed temporal patterns for {temporal_records} sample customers\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 3: UDF Best Practices and Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF best practices and common pitfalls\n",
    "print(\"üéØ UDF Best Practices and Troubleshooting\")\n",
    "\n",
    "# 1. Error handling in UDFs\n",
    "def safe_divide_udf(numerator, denominator):\n",
    "    \"\"\"Safe division with error handling\"\"\"\n",
    "    try:\n",
    "        if denominator == 0 or denominator is None:\n",
    "            return None\n",
    "        if numerator is None:\n",
    "            return None\n",
    "        return float(numerator) / float(denominator)\n",
    "    except (TypeError, ValueError, ZeroDivisionError):\n",
    "        return None\n",
    "\n",
    "safe_divide = udf(safe_divide_udf, DoubleType())\n",
    "\n",
    "# 2. Null handling patterns\n",
    "def handle_nulls_properly(value, default_value=0):\n",
    "    \"\"\"Proper null handling in UDFs\"\"\"\n",
    "    if value is None:\n",
    "        return default_value\n",
    "    try:\n",
    "        return int(value) * 2  # Example transformation\n",
    "    except (TypeError, ValueError):\n",
    "        return default_value\n",
    "\n",
    "null_safe_udf = udf(handle_nulls_properly, IntegerType())\n",
    "\n",
    "# 3. Performance monitoring UDF\n",
    "def monitored_udf_function(input_value):\n",
    "    \"\"\"UDF with performance monitoring\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Your business logic here\n",
    "    result = input_value.upper() if input_value else \"\"\n",
    "    \n",
    "    # Log performance (in production, use proper logging)\n",
    "    execution_time = time.time() - start_time\n",
    "    if execution_time > 0.001:  # Log slow operations\n",
    "        print(f\"Slow UDF execution: {execution_time:.4f}s for input: {input_value}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "monitored_udf = udf(monitored_udf_function, StringType())\n",
    "\n",
    "print(\"‚úÖ UDF Best Practices:\")\n",
    "print(\"1. Always handle None/null values\")\n",
    "print(\"2. Use appropriate error handling\")\n",
    "print(\"3. Prefer Pandas UDFs for numerical operations\")\n",
    "print(\"4. Avoid complex object serialization\")\n",
    "print(\"5. Monitor UDF performance\")\n",
    "print(\"6. Consider built-in functions first\")\n",
    "\n",
    "# Test safe operations\n",
    "test_data = spark.createDataFrame([\n",
    "    (10, 2, \"test\"),\n",
    "    (15, 0, \"hello\"),\n",
    "    (None, 5, None),\n",
    "    (20, None, \"world\")\n",
    "], [\"numerator\", \"denominator\", \"text_col\"])\n",
    "\n",
    "safe_operations_result = test_data \\\n",
    "    .withColumn(\"safe_division\", safe_divide(col(\"numerator\"), col(\"denominator\"))) \\\n",
    "    .withColumn(\"null_safe_transform\", null_safe_udf(col(\"numerator\"), lit(0))) \\\n",
    "    .withColumn(\"monitored_text\", monitored_udf(col(\"text_col\")))\n",
    "\n",
    "print(\"\\nüß™ Safe UDF Operations Test:\")\n",
    "safe_operations_result.show()\n",
    "\n",
    "# Common alternatives to UDFs\n",
    "print(\"\\nüöÄ UDF Alternatives (Often Better Performance):\")\n",
    "\n",
    "# Instead of UDF for simple conditions\n",
    "print(\"‚ùå UDF approach:\")\n",
    "def classify_amount_udf(amount):\n",
    "    if amount is None:\n",
    "        return \"unknown\"\n",
    "    elif amount > 1000:\n",
    "        return \"high\"\n",
    "    elif amount > 100:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "amount_classifier = udf(classify_amount_udf, StringType())\n",
    "\n",
    "print(\"‚úÖ Built-in functions approach:\")\n",
    "# Use when/otherwise instead\n",
    "transactions_classified = transactions_df.withColumn(\n",
    "    \"amount_category_builtin\",\n",
    "    when(col(\"amount\").isNull(), \"unknown\")\n",
    "    .when(col(\"amount\") > 1000, \"high\")\n",
    "    .when(col(\"amount\") > 100, \"medium\")\n",
    "    .otherwise(\"low\")\n",
    ")\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\n‚è±Ô∏è  Performance: Built-in vs UDF\")\n",
    "\n",
    "# UDF approach timing\n",
    "start_time = time.time()\n",
    "udf_count = transactions_df.withColumn(\"category_udf\", amount_classifier(col(\"amount\"))).count()\n",
    "udf_time = time.time() - start_time\n",
    "\n",
    "# Built-in approach timing\n",
    "start_time = time.time()\n",
    "builtin_count = transactions_classified.count()\n",
    "builtin_time = time.time() - start_time\n",
    "\n",
    "print(f\"UDF approach: {udf_count:,} records in {udf_time:.4f}s\")\n",
    "print(f\"Built-in approach: {builtin_count:,} records in {builtin_time:.4f}s\")\n",
    "\n",
    "if builtin_time > 0:\n",
    "    speedup = udf_time / builtin_time\n",
    "    print(f\"Built-in functions are {speedup:.1f}x faster!\")\n",
    "\n",
    "# Show sample results to verify correctness\n",
    "print(\"\\nüìä Sample categorization results:\")\n",
    "comparison_sample = transactions_df.withColumn(\"udf_category\", amount_classifier(col(\"amount\"))) \\\n",
    "    .withColumn(\"builtin_category\", \n",
    "        when(col(\"amount\").isNull(), \"unknown\")\n",
    "        .when(col(\"amount\") > 1000, \"high\")\n",
    "        .when(col(\"amount\") > 100, \"medium\")\n",
    "        .otherwise(\"low\")\n",
    "    ).select(\"amount\", \"udf_category\", \"builtin_category\")\n",
    "\n",
    "comparison_sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Implement robust, production-ready UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Production-Ready UDF Implementation Challenge\n",
    "\n",
    "# Challenge 1: Data Validation and Cleansing UDF\n",
    "def create_data_validator():\n",
    "    \"\"\"Factory function to create configurable data validation UDF\"\"\"\n",
    "    \n",
    "    @pandas_udf(returnType=StructType([\n",
    "        StructField(\"is_valid\", BooleanType()),\n",
    "        StructField(\"validation_errors\", StringType()),\n",
    "        StructField(\"cleansed_value\", StringType())\n",
    "    ]))\n",
    "    def validate_and_cleanse(input_series: pd.Series, \n",
    "                           validation_type: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"Comprehensive data validation and cleansing\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for value, v_type in zip(input_series, validation_type):\n",
    "            is_valid = True\n",
    "            errors = []\n",
    "            cleansed = str(value) if value is not None else \"\"\n",
    "            \n",
    "            try:\n",
    "                if v_type == 'email':\n",
    "                    # Email validation and cleansing\n",
    "                    if value is None or '@' not in str(value):\n",
    "                        is_valid = False\n",
    "                        errors.append(\"Invalid email format\")\n",
    "                    else:\n",
    "                        # Clean email (lowercase, trim)\n",
    "                        cleansed = str(value).lower().strip()\n",
    "                        # Basic email pattern check\n",
    "                        import re\n",
    "                        if not re.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', cleansed):\n",
    "                            is_valid = False\n",
    "                            errors.append(\"Invalid email pattern\")\n",
    "                \n",
    "                elif v_type == 'phone':\n",
    "                    # Phone number validation and formatting\n",
    "                    if value is None:\n",
    "                        is_valid = False\n",
    "                        errors.append(\"Missing phone number\")\n",
    "                    else:\n",
    "                        # Extract digits only\n",
    "                        digits = re.sub(r'[^0-9]', '', str(value))\n",
    "                        if len(digits) == 10:\n",
    "                            cleansed = f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\"\n",
    "                        elif len(digits) == 11 and digits[0] == '1':\n",
    "                            cleansed = f\"({digits[1:4]}) {digits[4:7]}-{digits[7:]}\"\n",
    "                        else:\n",
    "                            is_valid = False\n",
    "                            errors.append(\"Invalid phone number length\")\n",
    "                \n",
    "                elif v_type == 'currency':\n",
    "                    # Currency validation and normalization\n",
    "                    if value is None:\n",
    "                        is_valid = False\n",
    "                        errors.append(\"Missing currency value\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            # Remove currency symbols and convert to float\n",
    "                            clean_value = re.sub(r'[^0-9.-]', '', str(value))\n",
    "                            amount = float(clean_value)\n",
    "                            if amount < 0:\n",
    "                                errors.append(\"Negative currency value\")\n",
    "                            cleansed = f\"${amount:.2f}\"\n",
    "                        except ValueError:\n",
    "                            is_valid = False\n",
    "                            errors.append(\"Invalid currency format\")\n",
    "                \n",
    "                elif v_type == 'date':\n",
    "                    # Date validation and standardization\n",
    "                    if value is None:\n",
    "                        is_valid = False\n",
    "                        errors.append(\"Missing date\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            # Try to parse common date formats\n",
    "                            from datetime import datetime\n",
    "                            date_formats = ['%Y-%m-%d', '%m/%d/%Y', '%d-%m-%Y']\n",
    "                            parsed_date = None\n",
    "                            for fmt in date_formats:\n",
    "                                try:\n",
    "                                    parsed_date = datetime.strptime(str(value), fmt)\n",
    "                                    break\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                            \n",
    "                            if parsed_date:\n",
    "                                cleansed = parsed_date.strftime('%Y-%m-%d')\n",
    "                            else:\n",
    "                                is_valid = False\n",
    "                                errors.append(\"Invalid date format\")\n",
    "                        except Exception:\n",
    "                            is_valid = False\n",
    "                            errors.append(\"Date parsing error\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                is_valid = False\n",
    "                errors.append(f\"Validation error: {str(e)}\")\n",
    "            \n",
    "            results.append({\n",
    "                'is_valid': is_valid,\n",
    "                'validation_errors': '|'.join(errors) if errors else '',\n",
    "                'cleansed_value': cleansed\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    return validate_and_cleanse\n",
    "\n",
    "# Challenge 2: Advanced Business Rules Engine\n",
    "class BusinessRulesEngine:\n",
    "    \"\"\"Configurable business rules engine using UDFs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules = {}\n",
    "    \n",
    "    def add_rule(self, rule_name, rule_function):\n",
    "        \"\"\"Add a business rule\"\"\"\n",
    "        self.rules[rule_name] = rule_function\n",
    "    \n",
    "    def create_rules_udf(self):\n",
    "        \"\"\"Create UDF that applies all registered rules\"\"\"\n",
    "        \n",
    "        @pandas_udf(returnType=StructType([\n",
    "            StructField(\"rules_passed\", IntegerType()),\n",
    "            StructField(\"rules_failed\", IntegerType()),\n",
    "            StructField(\"failed_rules\", StringType()),\n",
    "            StructField(\"overall_status\", StringType())\n",
    "        ]))\n",
    "        def apply_business_rules(customer_data: pd.Series) -> pd.DataFrame:\n",
    "            \"\"\"Apply all business rules to customer data\"\"\"\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            for data_str in customer_data:\n",
    "                passed = 0\n",
    "                failed = 0\n",
    "                failed_rule_names = []\n",
    "                \n",
    "                try:\n",
    "                    # Parse customer data (assuming JSON-like string)\n",
    "                    if data_str and data_str != 'null':\n",
    "                        # Simple parsing for demo - in production use proper JSON parsing\n",
    "                        data_parts = str(data_str).split('|')\n",
    "                        data = {}\n",
    "                        for part in data_parts:\n",
    "                            if '=' in part:\n",
    "                                key, value = part.split('=', 1)\n",
    "                                try:\n",
    "                                    data[key] = float(value) if '.' in value else int(value)\n",
    "                                except ValueError:\n",
    "                                    data[key] = value\n",
    "                    else:\n",
    "                        data = {}\n",
    "                    \n",
    "                    # Apply rules\n",
    "                    for rule_name, rule_func in self.rules.items():\n",
    "                        try:\n",
    "                            if rule_func(data):\n",
    "                                passed += 1\n",
    "                            else:\n",
    "                                failed += 1\n",
    "                                failed_rule_names.append(rule_name)\n",
    "                        except Exception:\n",
    "                            failed += 1\n",
    "                            failed_rule_names.append(f\"{rule_name}_ERROR\")\n",
    "                \n",
    "                except Exception:\n",
    "                    # If parsing fails, mark all rules as failed\n",
    "                    failed = len(self.rules)\n",
    "                    failed_rule_names = [f\"{name}_PARSE_ERROR\" for name in self.rules.keys()]\n",
    "                \n",
    "                # Determine overall status\n",
    "                if failed == 0:\n",
    "                    status = \"APPROVED\"\n",
    "                elif failed <= 2:\n",
    "                    status = \"REVIEW\"\n",
    "                else:\n",
    "                    status = \"REJECTED\"\n",
    "                \n",
    "                results.append({\n",
    "                    'rules_passed': passed,\n",
    "                    'rules_failed': failed,\n",
    "                    'failed_rules': '|'.join(failed_rule_names),\n",
    "                    'overall_status': status\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(results)\n",
    "        \n",
    "        return apply_business_rules\n",
    "\n",
    "# Test the production-ready UDFs\n",
    "print(\"üè≠ Production-Ready UDF Testing\")\n",
    "\n",
    "# Test data validation UDF\n",
    "validator_udf = create_data_validator()\n",
    "\n",
    "# Create test dataset with various data quality issues\n",
    "test_validation_data = spark.createDataFrame([\n",
    "    (\"john@example.com\", \"email\"),\n",
    "    (\"invalid-email\", \"email\"),\n",
    "    (\"(555) 123-4567\", \"phone\"),\n",
    "    (\"5551234567\", \"phone\"),\n",
    "    (\"$123.45\", \"currency\"),\n",
    "    (\"-$50.00\", \"currency\"),\n",
    "    (\"2023-12-25\", \"date\"),\n",
    "    (\"12/25/2023\", \"date\"),\n",
    "    (\"invalid-date\", \"date\")\n",
    "], [\"input_value\", \"validation_type\"])\n",
    "\n",
    "# Apply validation UDF\n",
    "validated_data = test_validation_data.withColumn(\n",
    "    \"validation_result\",\n",
    "    validator_udf(col(\"input_value\"), col(\"validation_type\"))\n",
    ").select(\n",
    "    \"input_value\",\n",
    "    \"validation_type\",\n",
    "    col(\"validation_result.is_valid\").alias(\"is_valid\"),\n",
    "    col(\"validation_result.validation_errors\").alias(\"errors\"),\n",
    "    col(\"validation_result.cleansed_value\").alias(\"cleansed\")\n",
    ")\n",
    "\n",
    "print(\"üìã Data Validation Results:\")\n",
    "validated_data.show(truncate=False)\n",
    "\n",
    "# Test business rules engine\n",
    "rules_engine = BusinessRulesEngine()\n",
    "\n",
    "# Add business rules\n",
    "rules_engine.add_rule(\"min_age\", lambda data: data.get('age', 0) >= 18)\n",
    "rules_engine.add_rule(\"max_transaction\", lambda data: data.get('amount', 0) <= 10000)\n",
    "rules_engine.add_rule(\"valid_state\", lambda data: data.get('state', '') in ['CA', 'NY', 'TX', 'FL', 'WA', 'IL'])\n",
    "rules_engine.add_rule(\"positive_balance\", lambda data: data.get('balance', 0) >= 0)\n",
    "\n",
    "# Create test data for business rules\n",
    "rules_test_data = spark.createDataFrame([\n",
    "    (\"CUST_001\", \"age=25|amount=500|state=CA|balance=1000\"),\n",
    "    (\"CUST_002\", \"age=16|amount=200|state=NY|balance=500\"),\n",
    "    (\"CUST_003\", \"age=30|amount=15000|state=XX|balance=-100\"),\n",
    "    (\"CUST_004\", \"age=45|amount=750|state=TX|balance=2000\")\n",
    "], [\"customer_id\", \"customer_data\"])\n",
    "\n",
    "# Apply business rules\n",
    "rules_udf = rules_engine.create_rules_udf()\n",
    "customer_rules_test = rules_test_data.withColumn(\n",
    "    \"rules_result\",\n",
    "    rules_udf(col(\"customer_data\"))\n",
    ").select(\n",
    "    \"customer_id\",\n",
    "    col(\"rules_result.rules_passed\").alias(\"passed\"),\n",
    "    col(\"rules_result.rules_failed\").alias(\"failed\"),\n",
    "    col(\"rules_result.failed_rules\").alias(\"failed_rules\"),\n",
    "    col(\"rules_result.overall_status\").alias(\"status\")\n",
    ")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  Business Rules Results:\")\n",
    "customer_rules_test.show(truncate=False)\n",
    "\n",
    "# Performance and error monitoring\n",
    "print(\"\\nüìä UDF Performance Monitoring:\")\n",
    "\n",
    "# Time the validation UDF\n",
    "start_time = time.time()\n",
    "validation_count = validated_data.count()\n",
    "validation_time = time.time() - start_time\n",
    "\n",
    "# Time the business rules UDF\n",
    "start_time = time.time()\n",
    "rules_count = customer_rules_test.count()\n",
    "rules_time = time.time() - start_time\n",
    "\n",
    "print(f\"Data Validation UDF: {validation_count} records in {validation_time:.4f}s\")\n",
    "print(f\"Business Rules UDF: {rules_count} records in {rules_time:.4f}s\")\n",
    "\n",
    "# Summary statistics\n",
    "valid_records = validated_data.filter(col(\"is_valid\")).count()\n",
    "approved_customers = customer_rules_test.filter(col(\"status\") == \"APPROVED\").count()\n",
    "\n",
    "print(f\"\\nüìà Quality Metrics:\")\n",
    "print(f\"Data Validation: {valid_records}/{validation_count} records valid ({valid_records/validation_count*100:.1f}%)\")\n",
    "print(f\"Business Rules: {approved_customers}/{rules_count} customers approved ({approved_customers/rules_count*100:.1f}%)\")\n",
    "\n",
    "# Validation\n",
    "assert validation_count > 0, \"Should have validation results\"\n",
    "assert rules_count > 0, \"Should have business rules results\"\n",
    "assert valid_records >= 0, \"Should have valid record count\"\n",
    "assert approved_customers >= 0, \"Should have approved customer count\"\n",
    "\n",
    "print(f\"\\n‚úì Exercise 3.1 completed!\")\n",
    "print(f\"üè≠ Production-ready UDF pipeline implemented and tested\")\n",
    "print(f\"üîç Validated {validation_count} data points, processed {rules_count} business rule evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary: UDF Mastery\n",
    "\n",
    "### Key Capabilities Mastered:\n",
    "1. **UDF Creation**: Regular and Pandas UDFs for different use cases\n",
    "2. **Performance Optimization**: Vectorization with Pandas UDFs for significant speedups\n",
    "3. **Error Handling**: Robust null handling and exception management\n",
    "4. **Business Logic**: Complex domain-specific transformations\n",
    "5. **Production Patterns**: Validation, monitoring, and maintainable UDFs\n",
    "\n",
    "### Performance Guidelines:\n",
    "| **Use Case** | **Best Choice** | **Performance** | **When to Use** |\n",
    "|--------------|-----------------|----------------|----------------|\n",
    "| Simple conditions | Built-in functions | Fastest | Always prefer when possible |\n",
    "| Complex string operations | Regular UDF | Good | When built-ins insufficient |\n",
    "| Numerical computations | Pandas UDF | Very Good | Mathematical operations |\n",
    "| ML feature engineering | Pandas UDF | Excellent | Vectorizable operations |\n",
    "| Business rule validation | Pandas UDF | Very Good | Complex logic with multiple conditions |\n",
    "| External API calls | Regular UDF (with caution) | Varies | When unavoidable, use connection pooling |\n",
    "\n",
    "### Advanced Techniques Demonstrated:\n",
    "- ‚úÖ **RFM Analysis**: Customer segmentation with vectorized calculations\n",
    "- ‚úÖ **Temporal Pattern Analysis**: Time-series feature extraction\n",
    "- ‚úÖ **Data Validation Pipeline**: Production-ready validation with error handling\n",
    "- ‚úÖ **Business Rules Engine**: Configurable rule evaluation system\n",
    "- ‚úÖ **Performance Monitoring**: UDF execution time tracking\n",
    "- ‚úÖ **Error Recovery**: Graceful handling of malformed data\n",
    "\n",
    "### Best Practices Checklist:\n",
    "- ‚úÖ Handle null values explicitly in all UDFs\n",
    "- ‚úÖ Use appropriate return types with proper schema definition\n",
    "- ‚úÖ Prefer Pandas UDFs for numerical operations (3-10x faster)\n",
    "- ‚úÖ Consider built-in functions first - always benchmark\n",
    "- ‚úÖ Implement comprehensive error handling with try-catch blocks\n",
    "- ‚úÖ Monitor UDF performance and log slow operations\n",
    "- ‚úÖ Test UDFs thoroughly with edge cases and invalid data\n",
    "- ‚úÖ Document complex business logic with clear examples\n",
    "- ‚úÖ Use factory patterns for configurable UDFs\n",
    "- ‚úÖ Implement proper data type validation and conversion\n",
    "\n",
    "### Production Deployment Considerations:\n",
    "- **Serialization**: Minimize complex object passing to UDFs\n",
    "- **Resource Management**: Monitor memory usage with large datasets\n",
    "- **Error Logging**: Implement proper logging for production debugging\n",
    "- **Version Control**: Track UDF changes and maintain backward compatibility\n",
    "- **Testing**: Create comprehensive test suites for business logic validation\n",
    "- **Performance**: Regular benchmarking against built-in alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"üéâ Lab 7 completed! UDF expertise achieved.\")\n",
    "print(\"üèÜ Module 1 Lab Series Complete!\")\n",
    "print(\"\\nüìö You've mastered:\")\n",
    "print(\"  ‚úÖ RDD Fundamentals - Core distributed data structures\")\n",
    "print(\"  ‚úÖ Transformations vs Actions - Lazy evaluation principles\")\n",
    "print(\"  ‚úÖ Lazy Evaluation - DAG optimization and Catalyst engine\")\n",
    "print(\"  ‚úÖ DataFrame API - Structured data processing\")\n",
    "print(\"  ‚úÖ Spark SQL - Complex analytical queries\")\n",
    "print(\"  ‚úÖ DataFrame Operations - Advanced joins and aggregations\")\n",
    "print(\"  ‚úÖ User-Defined Functions - Custom business logic\")\n",
    "print(\"\\nüöÄ Ready for Module 2: Advanced Spark Techniques!\")\n",
    "print(\"üí° Next topics: Streaming, MLlib, Performance Tuning, Production Deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}